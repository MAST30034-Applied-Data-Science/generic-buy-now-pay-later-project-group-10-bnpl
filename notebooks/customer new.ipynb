{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "#==============================================================================\n",
    "import lbl2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import max, lit\n",
    "from pyspark.sql.functions import date_format\n",
    "import statsmodels.api as sm\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from pyspark.sql.functions import year, month\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import outlier\n",
    "#==============================================================================\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2 part 8\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../scripts/ETL.py\" '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../scripts/outlier.py\" '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# STEP 1: Prepare the main dataset\n",
    "#==============================================================================\n",
    "# Read the tagged model\n",
    "tagged_merchants_sdf = spark.read.parquet(\"../data/curated/tagged_merchants.parquet\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Rename the merchant column \n",
    "tagged_merchants_sdf = tagged_merchants_sdf.withColumnRenamed('merchant_abn',\n",
    "\n",
    "    'tagged_merchant_abn'\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join the final dataset to the tagged model\n",
    "internal4.createOrReplaceTempView(\"join\")\n",
    "tagged_merchants_sdf.createOrReplaceTempView(\"tagged\")\n",
    "\n",
    "joint = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM join\n",
    "INNER JOIN tagged\n",
    "ON join.merchant_abn = tagged.tagged_merchant_abn\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculate the BNPL earnings \n",
    "joint = joint.drop('tagged_merchant_abn')\n",
    "joint.createOrReplaceTempView(\"group\")\n",
    "\n",
    "a_customer = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *, (take_rate/100)*dollar_value AS percent\n",
    "FROM group\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Extracting the year, month, day from the timestamp\n",
    "a_customer = a_customer.withColumn('Year', year(a_customer.order_datetime))\n",
    "a_customer = a_customer.withColumn('Month',month(a_customer.order_datetime))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop the unwanted columns\n",
    "a_customer = a_customer.drop('merchant_abn', 'categories','name', 'address', 'trans_merchant_abn', \n",
    "'order_id','order_datetime', 'consumer_id','int_sa2', 'SA2_name','state_code',\n",
    "'state_name','population_2020', 'population_2021')\n",
    " \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Find Count of Null, None, NaN of All DataFrame Columns\n",
    "a_customer.select([count(when(isnan(c) | col(c).isNull(), \n",
    "c)).alias(c) for c in a_customer.columns])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculate the number of male and females customers for each merchant\n",
    "a_customer.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "male = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS m_name, \n",
    "    COUNT(gender) as males\n",
    "FROM agg\n",
    "WHERE gender = 'Male'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "female = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS f_name, \n",
    "    COUNT(gender) as females\n",
    "FROM agg\n",
    "WHERE gender = 'Female'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Aggregate the main dataset\n",
    "a_customer.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "temp_customer = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, COUNT(DISTINCT user_id) AS no_of_customers, SA2_code, \n",
    "Year, Month, SUM(dollar_value - percent) AS total_earnings,\n",
    "    CONCAT(merchant_name, SA2_code, Year, Month) AS join_col\n",
    "FROM agg \n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the male and female customer counts to the main dataset\n",
    "temp_customer.createOrReplaceTempView(\"gender_join\")\n",
    "male.createOrReplaceTempView(\"m\")\n",
    "female.createOrReplaceTempView(\"f\")\n",
    "\n",
    "temp2_customer = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM gender_join\n",
    "INNER JOIN m\n",
    "ON gender_join.join_col = m.m_name\n",
    "\"\"\")\n",
    "\n",
    "temp2_customer.createOrReplaceTempView(\"temp2\")\n",
    "\n",
    "temp3_customer = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM temp2\n",
    "INNER JOIN f\n",
    "ON temp2.join_col = f.f_name\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Rename the column for better readability \n",
    "a_customer = a_customer.withColumnRenamed('income_2018-2019',\n",
    "\n",
    "    'income_2018_2019'    \n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculate the income per person for each SA2 code\n",
    "a_customer = a_customer.withColumn('income_per_persons',\n",
    "    (F.col('income_2018_2019')/F.col('total_persons'))\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Extract the revenue level and category for each merchant and total females,\n",
    "# males and income per person for each SA2 code\n",
    "a_customer.createOrReplaceTempView(\"features\")\n",
    "\n",
    "e_customer = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name AS drop_name, FIRST(take_rate) AS take_rate, \n",
    "    FIRST(revenue_levels) AS revenue_levels, FIRST(category) AS category,\n",
    "    FIRST(total_males) AS males_in_SA2, FIRST(total_females) AS females_in_SA2, \n",
    "    FIRST(income_per_persons) AS income_per_person\n",
    "FROM features\n",
    "GROUP BY merchant_name\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the extracted values to the main dataset\n",
    "temp3_customer.createOrReplaceTempView(\"edit\")\n",
    "e_customer.createOrReplaceTempView(\"rates\")\n",
    "\n",
    "temp4_customer = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM edit\n",
    "INNER JOIN rates\n",
    "ON edit.merchant_name = rates.drop_name\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop the redundant columns\n",
    "train_customer = temp4_customer.drop('m_name', 'f_name', 'drop_name','join_col')\n",
    "\n",
    "#==============================================================================\n",
    "# STEP 2: Prepare a train and test dataset by offsetting the months by 1\n",
    "#==============================================================================\n",
    "# ----------------------------------------------------------------------------\n",
    "# Select the main columns for offsetting\n",
    "train_projection_customer = train_customer.select(\"merchant_name\", \"SA2_code\", \"Year\", \n",
    "\"Month\", 'no_of_customers')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Offset the dataset by 1 month\n",
    "\n",
    "# Offset the year by 1 if the month if the first month\n",
    "train_projection_customer = train_projection_customer.withColumn(\"prev_year\", \\\n",
    "              when(train_projection_customer[\"Month\"] == 1, \n",
    "              train_projection_customer['Year'] - 1).otherwise(train_projection_customer['Year']))\n",
    "train_projection_customer = train_projection_customer.withColumn(\"prev_month\", \\\n",
    "              when(train_projection_customer[\"Month\"] == 1, 12\n",
    "              ).otherwise(train_projection_customer['Month'] - 1))\n",
    "\n",
    "# Drop the redundant columns\n",
    "train_projection_customer = train_projection_customer.drop(\"Year\", \"Month\")\n",
    "\n",
    "# Renam the columns\n",
    "train_projection_customer = train_projection_customer.withColumnRenamed(\"no_of_customers\", \n",
    "            \"future_customers\") \\\n",
    "            .withColumnRenamed(\"merchant_name\", \n",
    "            \"p_merchant_name\") \\\n",
    "            .withColumnRenamed(\"SA2_code\", \"p_SA2_code\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the offsetted values to the rest of the SA2 and aggregated values\n",
    "final_data_customer = train_customer.join(train_projection_customer, \n",
    "            (train_customer.merchant_name == train_projection_customer.p_merchant_name) & \n",
    "            (train_customer.SA2_code == train_projection_customer.p_SA2_code) & \n",
    "            (train_customer.Year == train_projection_customer.prev_year) & \n",
    "            (train_customer.Month == train_projection_customer.prev_month), how = 'inner')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop the redundant columns\n",
    "final_data_customer = final_data_customer.drop(\"p_merchant_name\", \"p_SA2_code\",\"prev_year\", \n",
    "\"prev_month\")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Change the variable types\n",
    "field_str_customer = ['Year', 'Month', 'SA2_code']\n",
    "\n",
    "for cols in field_str_customer:\n",
    "    final_data_customer = final_data_customer.withColumn(cols,F.col(cols).cast('STRING'))\n",
    "\n",
    "field_int_customer = ['no_of_customers', 'males', 'females', 'males_in_SA2', \n",
    "'females_in_SA2']\n",
    "\n",
    "for col in field_int_customer:\n",
    "    final_data_customer = final_data_customer.withColumn(col, F.col(col).cast('INT'))\n",
    "\n",
    "#==============================================================================\n",
    "# STEP 3: Build and train the Random Forrest Model\n",
    "#==============================================================================\n",
    "# String indexing the categorical columns\n",
    "\n",
    "field = ['future_customers','no_of_customers' ,'males', 'females', \n",
    "        'males_in_SA2', 'females_in_SA2']\n",
    "\n",
    "for col in field:\n",
    "    final_data_customer = final_data_customer.withColumn(col,\n",
    "\n",
    "    F.col(col).cast('INT')\n",
    "\n",
    ")\n",
    "# String indexing the categorical columns\n",
    "\n",
    "indexer_customer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', \n",
    "'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', \n",
    "'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data_customer = indexer_customer.fit(final_data_customer).transform(final_data_customer)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder_customer = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', \n",
    "'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata_customer = encoder_customer.fit(indexd_data_customer).transform(indexd_data_customer)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1_customer = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2',\n",
    "'no_of_customers' ,'income_per_person','take_rate', 'total_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1_customer = assembler1_customer.transform(onehotdata_customer)\n",
    "# Renaming the target column as label\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Renaming the target column as label\n",
    "\n",
    "outdata1_customer = outdata1_customer.withColumnRenamed(\n",
    "    \"future_customers\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Assembling the features as a feature vector \n",
    "featureIndexer_customer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1_customer)\n",
    "\n",
    "outdata1_customer = featureIndexer_customer.transform(outdata1_customer)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "\n",
    "trainingData_customer, testData_customer = outdata1_customer.randomSplit([0.7, 0.3], seed = 20)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Train model.  \n",
    "model_customer = rf.fit(trainingData_customer)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Make predictions.\n",
    "predictions_validation_customer = model_customer.transform(testData_customer)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Evaluate the validation set \n",
    "\n",
    "predictions_validation_customer.select(\"prediction\", \"label\", \"features\")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Select (prediction, true label) and compute test error\n",
    "\n",
    "evaluator_train_rmse_customer = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_train_customer = evaluator_train_rmse_customer.evaluate(predictions_validation_customer)\n",
    "\n",
    "evaluator_train_mae_customer = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae_train_customer = evaluator_train_mae_customer.evaluate(predictions_validation_customer)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Define a function to extract important feature column names\n",
    "def ExtractFeatureImportance(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol\n",
    "        ].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "  \n",
    "  # ---------------------------------------------------------------------------\n",
    "#ExtractFeatureImportance(model.stages[-1].featureImportances, dataset, \n",
    "# \"features\")\n",
    "dataset_fi_customer = ExtractFeatureImportance(model_customer.featureImportances, \n",
    "predictions_validation_customer, \"features\")\n",
    "dataset_fi_customer = spark.createDataFrame(dataset_fi_customer)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Select the latest month from the latest year in the dataset which will be\n",
    "# used as a test set for future predictions due to the offsetting done \n",
    "# previously\n",
    "\n",
    "latest_year_customer = train_customer.select(max('Year')).collect()[0][0]\n",
    "agg_month_1_customer = train_customer.filter(train_customer.Year == latest_year_customer)\n",
    "latest_month_customer = agg_month_1_customer.select(max('Month')).collect()[0][0]\n",
    "predicting_data_customer = agg_month_1_customer.filter(train_customer.Month == latest_month_customer)\n",
    "predicting_data_customer = predicting_data_customer.withColumn(\"future_customers\", lit(0))\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Change the variable types\n",
    "field_str_customer = ['Year', 'Month', 'SA2_code']\n",
    "\n",
    "for cols in field_str_customer:\n",
    "    predicting_data_customer = predicting_data_customer.withColumn(cols,F.col(cols).cast('STRING'))\n",
    "\n",
    "field_int_customer = ['no_of_customers', 'males', 'females', 'males_in_SA2', \n",
    "'females_in_SA2']\n",
    "\n",
    "for col in field_int_customer:\n",
    "    predicting_data_customer = predicting_data_customer.withColumn(col, F.col(col).cast('INT'))\n",
    "#==============================================================================\n",
    "# STEP 4: Make future predictions\n",
    "#==============================================================================\n",
    "# Repeat the indexing and vector assembling steps again for the test data\n",
    "# String indexing the categorical columns\n",
    "indexer_customer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', \n",
    "'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', \n",
    "'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data_customer = indexer_customer.fit(predicting_data_customer).transform(predicting_data_customer)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder_customer = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', \n",
    "'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata_customer = encoder_customer.fit(indexd_data_customer).transform(indexd_data_customer)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1_customer = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', \n",
    "'income_per_person', 'no_of_customers','take_rate', 'total_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1_customer = assembler1_customer.transform(onehotdata_customer)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Renaming the target column as label\n",
    "\n",
    "outdata1_customer = outdata1_customer.withColumnRenamed(\n",
    "    \"future_customers\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Assembling the features as a feature vector \n",
    "featureIndexer_customer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1_customer)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Transform the test data\n",
    "outdata1_customer = featureIndexer_customer.transform(outdata1_customer)\n",
    "predictions_test_customer = model_customer.transform(outdata1_customer)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Aggregate the predictions to merchant level to get the predicted BNPL \n",
    "# earnings from each merchant\n",
    "predictions_test_customer.createOrReplaceTempView(\"preds\")\n",
    "\n",
    "pred_customer = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, ROUND(SUM(prediction)) AS total_future_customers\n",
    "FROM preds\n",
    "GROUP BY merchant_name\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------  \n",
    "# Convert the predictions to a pandas dataframe and save as a csv\n",
    "pred_df_customer = pred_customer.toPandas()\n",
    "pred_df_customer.to_csv(\"../data/curated/customers.csv\")\n",
    "# ----------------------------------------------------------------------------- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
