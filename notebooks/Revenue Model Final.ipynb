{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import year, month\n",
    "import pandas as pd\n",
    "import lbl2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:40:22 WARN Utils: Your hostname, MacBook-Air-3.local resolves to a loopback address: 127.0.0.1; using 192.168.0.66 instead (on interface en0)\n",
      "22/10/06 13:40:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:40:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/06 13:40:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/06 13:40:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kasturi/opt/anaconda3/lib/python3.9/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:40:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read in data from ETL.py file\n",
    "%run '../scripts/ETL.py' '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_merchants_sdf = spark.read.parquet(\"../data/curated/tagged_merchants.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_merchants_sdf = tagged_merchants_sdf.withColumnRenamed('merchant_abn',\n",
    "\n",
    "    'tagged_merchant_abn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:41:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "final_join3.createOrReplaceTempView(\"join\")\n",
    "tagged_merchants_sdf.createOrReplaceTempView(\"tagged\")\n",
    "\n",
    "joint = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM join\n",
    "INNER JOIN tagged\n",
    "ON join.merchant_abn = tagged.tagged_merchant_abn\n",
    "\"\"\")\n",
    "\n",
    "joint = joint.drop('tagged_merchant_abn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint.createOrReplaceTempView(\"group\")\n",
    "\n",
    "main_data = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *, ((take_rate/100)*dollar_value) AS percent\n",
    "FROM group\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year, month, day from the timestamp\n",
    "\n",
    "\n",
    "main_data = main_data.withColumn('Year', year(main_data.order_datetime))\n",
    "main_data = main_data.withColumn('Month',month(main_data.order_datetime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = main_data.drop('merchant_abn', 'categories','name', 'address', 'trans_merchant_abn', 'order_id','order_datetime','user_id',\n",
    "'consumer_id','int_sa2','SA2_name','state_code','state_name','population_2020', 'population_2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_name</th><th>take_rate</th><th>revenue_levels</th><th>state</th><th>gender</th><th>dollar_value</th><th>postcodes</th><th>SA2_code</th><th>income_2018-2019</th><th>total_males</th><th>total_females</th><th>total_persons</th><th>category</th><th>percent</th><th>Year</th><th>Month</th></tr>\n",
       "<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+---------+--------------+-----+------+------------+---------+--------+----------------+-----------+-------------+-------------+--------+-------+----+-----+\n",
       "|merchant_name|take_rate|revenue_levels|state|gender|dollar_value|postcodes|SA2_code|income_2018-2019|total_males|total_females|total_persons|category|percent|Year|Month|\n",
       "+-------------+---------+--------------+-----+------+------------+---------+--------+----------------+-----------+-------------+-------------+--------+-------+----+-----+\n",
       "|            0|        0|             0|    0|     0|           0|        0|       0|               0|          0|            0|            0|       0|      0|   0|    0|\n",
       "+-------------+---------+--------------+-----+------+------------+---------+--------+----------------+-----------+-------------+-------------+--------+-------+----+-----+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Find Count of Null, None, NaN of All DataFrame Columns\n",
    "main_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in main_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "male = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS m_name, COUNT(gender) as males\n",
    "FROM agg\n",
    "WHERE gender = 'Male'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "female = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS f_name, COUNT(gender) as females\n",
    "FROM agg\n",
    "WHERE gender = 'Female'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "main_agg_data = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, COUNT(merchant_name) AS no_of_transactions, SA2_code, Year, Month, SUM(dollar_value - percent) AS total_earnings,\n",
    "    CONCAT(merchant_name, SA2_code, Year, Month) AS join_col\n",
    "FROM agg\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_agg_data.createOrReplaceTempView(\"gender_join\")\n",
    "male.createOrReplaceTempView(\"male_agg\")\n",
    "female.createOrReplaceTempView(\"female_agg\")\n",
    "\n",
    "temp = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM gender_join\n",
    "INNER JOIN male_agg\n",
    "ON gender_join.join_col = male_agg.m_name\n",
    "\"\"\")\n",
    "\n",
    "temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "gender_agg = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM temp\n",
    "INNER JOIN female_agg\n",
    "ON temp.join_col = female_agg.f_name\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = main_data.withColumnRenamed('income_2018-2019',\n",
    "\n",
    "    'income_2018_2019'    \n",
    ")\n",
    "\n",
    "main_data = main_data.withColumn('income_per_persons',\n",
    "    (F.col('income_2018_2019')/F.col('total_persons'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.createOrReplaceTempView(\"features\")\n",
    "\n",
    "other_agg = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name AS drop_name, FIRST(take_rate) AS take_rate, FIRST(revenue_levels) AS revenue_levels, FIRST(category) AS category,\n",
    "    FIRST(total_males) AS males_in_SA2, FIRST(total_females) AS females_in_SA2, FIRST(income_per_persons) AS income_per_person\n",
    "FROM features\n",
    "GROUP BY merchant_name\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_name</th><th>no_of_transactions</th><th>SA2_code</th><th>Year</th><th>Month</th><th>total_earnings</th><th>males</th><th>females</th><th>take_rate</th><th>revenue_levels</th><th>category</th><th>males_in_SA2</th><th>females_in_SA2</th><th>income_per_person</th></tr>\n",
       "<tr><td>A Aliquet Ltd</td><td>2</td><td>312021351</td><td>2021</td><td>6</td><td>298.8537411028336</td><td>1</td><td>1</td><td>3.87</td><td>b</td><td>Furniture</td><td>3292</td><td>3206</td><td>28693.71558221812</td></tr>\n",
       "<tr><td>A Aliquet Ltd</td><td>2</td><td>401021010</td><td>2021</td><td>4</td><td>670.4857203238805</td><td>1</td><td>1</td><td>3.87</td><td>b</td><td>Furniture</td><td>3292</td><td>3206</td><td>28693.71558221812</td></tr>\n",
       "<tr><td>A Aliquet Ltd</td><td>2</td><td>603011065</td><td>2021</td><td>12</td><td>346.5799250465364</td><td>1</td><td>1</td><td>3.87</td><td>b</td><td>Furniture</td><td>3292</td><td>3206</td><td>28693.71558221812</td></tr>\n",
       "<tr><td>A Arcu Industries</td><td>2</td><td>124011453</td><td>2021</td><td>8</td><td>203.5414742977921</td><td>1</td><td>1</td><td>3.0</td><td>c</td><td>Furniture</td><td>4821</td><td>4683</td><td>25816.03452631579</td></tr>\n",
       "<tr><td>A Arcu Industries</td><td>2</td><td>211051282</td><td>2022</td><td>3</td><td>655.1195924003883</td><td>1</td><td>1</td><td>3.0</td><td>c</td><td>Furniture</td><td>4821</td><td>4683</td><td>25816.03452631579</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+------------------+---------+----+-----+-----------------+-----+-------+---------+--------------+---------+------------+--------------+-----------------+\n",
       "|    merchant_name|no_of_transactions| SA2_code|Year|Month|   total_earnings|males|females|take_rate|revenue_levels| category|males_in_SA2|females_in_SA2|income_per_person|\n",
       "+-----------------+------------------+---------+----+-----+-----------------+-----+-------+---------+--------------+---------+------------+--------------+-----------------+\n",
       "|    A Aliquet Ltd|                 2|312021351|2021|    6|298.8537411028336|    1|      1|     3.87|             b|Furniture|        3292|          3206|28693.71558221812|\n",
       "|    A Aliquet Ltd|                 2|401021010|2021|    4|670.4857203238805|    1|      1|     3.87|             b|Furniture|        3292|          3206|28693.71558221812|\n",
       "|    A Aliquet Ltd|                 2|603011065|2021|   12|346.5799250465364|    1|      1|     3.87|             b|Furniture|        3292|          3206|28693.71558221812|\n",
       "|A Arcu Industries|                 2|124011453|2021|    8|203.5414742977921|    1|      1|      3.0|             c|Furniture|        4821|          4683|25816.03452631579|\n",
       "|A Arcu Industries|                 2|211051282|2022|    3|655.1195924003883|    1|      1|      3.0|             c|Furniture|        4821|          4683|25816.03452631579|\n",
       "+-----------------+------------------+---------+----+-----+-----------------+-----+-------+---------+--------------+---------+------------+--------------+-----------------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_agg.createOrReplaceTempView(\"edit\")\n",
    "other_agg.createOrReplaceTempView(\"rates\")\n",
    "\n",
    "other_cols = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM edit\n",
    "INNER JOIN rates\n",
    "ON edit.merchant_name = rates.drop_name\n",
    "\"\"\")\n",
    "\n",
    "train = other_cols.drop('m_name', 'f_name', 'drop_name','join_col')\n",
    "\n",
    "train.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_projection = train.select(\"merchant_name\", \"SA2_code\", \"Year\", \"Month\", 'total_earnings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_projection = train_projection.withColumn(\"prev_year\", \\\n",
    "              when(train_projection[\"Month\"] == 1, train_projection['Year'] - 1).otherwise(train_projection['Year']))\n",
    "train_projection = train_projection.withColumn(\"prev_month\", \\\n",
    "              when(train_projection[\"Month\"] == 1, 12).otherwise(train_projection['Month'] - 1))\n",
    "train_projection = train_projection.drop(\"Year\", \"Month\")\n",
    "train_projection = train_projection.withColumnRenamed(\"total_earnings\", \"future_earnings\") \\\n",
    "                            .withColumnRenamed(\"merchant_name\", \"p_merchant_name\") \\\n",
    "                            .withColumnRenamed(\"SA2_code\", \"p_SA2_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data= train.join(train_projection, (train.merchant_name == train_projection.p_merchant_name) & \n",
    "                           (train.SA2_code == train_projection.p_SA2_code) & \n",
    "                           (train.Year == train_projection.prev_year) & \n",
    "                           (train.Month == train_projection.prev_month), how = 'inner')\n",
    "\n",
    "final_data = final_data.drop(\"p_merchant_name\", \"p_SA2_code\",\"prev_year\", \"prev_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_str = ['Year', 'Month', 'SA2_code']\n",
    "\n",
    "for cols in field_str:\n",
    "    final_data = final_data.withColumn(cols,\n",
    "\n",
    "    F.col(cols).cast('STRING')\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "field_int = ['no_of_transactions', 'males', 'females', 'males_in_SA2', 'females_in_SA2']\n",
    "\n",
    "for col in field_int:\n",
    "    final_data = final_data.withColumn(col,\n",
    "\n",
    "    F.col(col).cast('INT')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# String indexing the categorical columns\n",
    "\n",
    "indexer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', 'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', 'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data = indexer.fit(final_data).transform(final_data)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', 'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1 = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', 'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', 'income_per_person', 'no_of_transactions','take_rate', 'total_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1 = assembler1.transform(onehotdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the target column as label\n",
    "\n",
    "outdata1 = outdata1.withColumnRenamed(\n",
    "    \"future_earnings\",\n",
    "    \"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assembling the features as a feature vector \n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1)\n",
    "\n",
    "outdata1 = featureIndexer.transform(outdata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "\n",
    "trainingData, testData = outdata1.randomSplit([0.7, 0.3], seed = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 496:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:46:37 WARN DAGScheduler: Broadcasting large task binary with size 1337.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:46:38 WARN DAGScheduler: Broadcasting large task binary with size 1337.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:46:40 WARN DAGScheduler: Broadcasting large task binary with size 1341.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:46:42 WARN DAGScheduler: Broadcasting large task binary with size 1454.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:46:48 WARN DAGScheduler: Broadcasting large task binary with size 1515.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:46:51 WARN DAGScheduler: Broadcasting large task binary with size 1636.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:46:56 WARN DAGScheduler: Broadcasting large task binary with size 1876.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:46:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "\n",
    "# Train model.  \n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_validation = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 755:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:48:08 WARN DAGScheduler: Broadcasting large task binary with size 1349.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|        prediction|             label|            features|\n",
      "+------------------+------------------+--------------------+\n",
      "|342.99433179557315| 175.5058926208255|(2085,[209,1093,2...|\n",
      "|342.99433179557315| 184.5112457341748|(2085,[209,988,20...|\n",
      "|342.99433179557315| 273.6255392020826|(2085,[209,976,20...|\n",
      "|342.99433179557315| 421.7745544721037|(2085,[209,989,20...|\n",
      "|342.99433179557315|152.81812360693857|(2085,[209,1039,2...|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 861:===============================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:49:25 WARN DAGScheduler: Broadcasting large task binary with size 1342.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 882:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:49:27 WARN DAGScheduler: Broadcasting large task binary with size 1343.9 KiB\n",
      "Root Mean Squared Error (RMSE) on train data = 339.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 968:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:50:42 WARN DAGScheduler: Broadcasting large task binary with size 1342.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 989:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 13:50:45 WARN DAGScheduler: Broadcasting large task binary with size 1343.9 KiB\n",
      "Mean Absolutee Error (MAE) on train data = 208.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the validation set \n",
    "\n",
    "predictions_validation.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "\n",
    "evaluator_train_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_train = evaluator_train_rmse.evaluate(predictions_validation)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data = %g\" % rmse_train)\n",
    "\n",
    "evaluator_train_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae_train = evaluator_train_mae.evaluate(predictions_validation)\n",
    "print(\"Mean Absolutee Error (MAE) on train data = %g\" % mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>idx</th><th>name</th><th>score</th></tr>\n",
       "<tr><td>2084</td><td>total_earnings</td><td>0.6636336838909377</td></tr>\n",
       "<tr><td>2082</td><td>no_of_transactions</td><td>0.15877433581190895</td></tr>\n",
       "<tr><td>2078</td><td>category_vec_Beau...</td><td>0.03629700211840285</td></tr>\n",
       "<tr><td>2074</td><td>category_vec_Book...</td><td>0.032823799638621705</td></tr>\n",
       "<tr><td>2076</td><td>category_vec_Furn...</td><td>0.011698635257700485</td></tr>\n",
       "<tr><td>110</td><td>merchant_name_vec...</td><td>0.011309588712838327</td></tr>\n",
       "<tr><td>2083</td><td>take_rate</td><td>0.009958495074833078</td></tr>\n",
       "<tr><td>975</td><td>SA2_code_vec_4060...</td><td>0.006728057018102661</td></tr>\n",
       "<tr><td>2079</td><td>males_in_SA2</td><td>0.006252293498002...</td></tr>\n",
       "<tr><td>274</td><td>merchant_name_vec...</td><td>0.004555797073978013</td></tr>\n",
       "<tr><td>2080</td><td>females_in_SA2</td><td>0.004377129722624831</td></tr>\n",
       "<tr><td>69</td><td>merchant_name_vec...</td><td>0.003900847588394871</td></tr>\n",
       "<tr><td>62</td><td>merchant_name_vec...</td><td>0.003879371920643...</td></tr>\n",
       "<tr><td>93</td><td>merchant_name_vec...</td><td>0.003826314446640767</td></tr>\n",
       "<tr><td>66</td><td>merchant_name_vec...</td><td>0.003461924248766236</td></tr>\n",
       "<tr><td>2081</td><td>income_per_person</td><td>0.003284183676310...</td></tr>\n",
       "<tr><td>2069</td><td>revenue_levels_vec_a</td><td>0.003152801510966...</td></tr>\n",
       "<tr><td>976</td><td>SA2_code_vec_5090...</td><td>0.003003877295812...</td></tr>\n",
       "<tr><td>2066</td><td>Month_vec_12</td><td>0.002835619942068...</td></tr>\n",
       "<tr><td>2075</td><td>category_vec_Elec...</td><td>0.002306671220630082</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----+--------------------+--------------------+\n",
       "| idx|                name|               score|\n",
       "+----+--------------------+--------------------+\n",
       "|2084|      total_earnings|  0.6636336838909377|\n",
       "|2082|  no_of_transactions| 0.15877433581190895|\n",
       "|2078|category_vec_Beau...| 0.03629700211840285|\n",
       "|2074|category_vec_Book...|0.032823799638621705|\n",
       "|2076|category_vec_Furn...|0.011698635257700485|\n",
       "| 110|merchant_name_vec...|0.011309588712838327|\n",
       "|2083|           take_rate|0.009958495074833078|\n",
       "| 975|SA2_code_vec_4060...|0.006728057018102661|\n",
       "|2079|        males_in_SA2|0.006252293498002...|\n",
       "| 274|merchant_name_vec...|0.004555797073978013|\n",
       "|2080|      females_in_SA2|0.004377129722624831|\n",
       "|  69|merchant_name_vec...|0.003900847588394871|\n",
       "|  62|merchant_name_vec...|0.003879371920643...|\n",
       "|  93|merchant_name_vec...|0.003826314446640767|\n",
       "|  66|merchant_name_vec...|0.003461924248766236|\n",
       "|2081|   income_per_person|0.003284183676310...|\n",
       "|2069|revenue_levels_vec_a|0.003152801510966...|\n",
       "| 976|SA2_code_vec_5090...|0.003003877295812...|\n",
       "|2066|        Month_vec_12|0.002835619942068...|\n",
       "|2075|category_vec_Elec...|0.002306671220630082|\n",
       "+----+--------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ExtractFeatureImportance(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "  \n",
    "  \n",
    "#ExtractFeatureImportance(model.stages[-1].featureImportances, dataset, \"features\")\n",
    "dataset_fi = ExtractFeatureImportance(model.featureImportances, predictions_validation, \"features\")\n",
    "dataset_fi = spark.createDataFrame(dataset_fi)\n",
    "display(dataset_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "latest_year = train.select(max('Year')).collect()[0][0]\n",
    "agg_month_1 = train.filter(train.Year == latest_year)\n",
    "latest_month = agg_month_1.select(max('Month')).collect()[0][0]\n",
    "predicting_data = agg_month_1.filter(train.Month == latest_month)\n",
    "predicting_data = predicting_data.withColumn(\"future_earnings\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# String indexing the categorical columns\n",
    "\n",
    "indexer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', 'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', 'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data = indexer.fit(predicting_data).transform(predicting_data)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', 'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1 = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', 'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', 'income_per_person', 'no_of_transactions','take_rate', 'total_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1 = assembler1.transform(onehotdata)\n",
    "\n",
    "# Renaming the target column as label\n",
    "\n",
    "outdata1 = outdata1.withColumnRenamed(\n",
    "    \"future_earnings\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "\n",
    "# Assembling the features as a feature vector \n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1)\n",
    "\n",
    "outdata1 = featureIndexer.transform(outdata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_name</th><th>total_revenue</th></tr>\n",
       "<tr><td>Dictum Mi Incorpo...</td><td>637.2824011283511</td></tr>\n",
       "<tr><td>Dictum Mi Limited</td><td>17206.62483046548</td></tr>\n",
       "<tr><td>Donec Luctus Indu...</td><td>4460.976807898458</td></tr>\n",
       "<tr><td>Elit Sed Consequa...</td><td>14657.495225952076</td></tr>\n",
       "<tr><td>Hendrerit Consect...</td><td>2867.77080507758</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------------+\n",
       "|       merchant_name|     total_revenue|\n",
       "+--------------------+------------------+\n",
       "|Dictum Mi Incorpo...| 637.2824011283511|\n",
       "|   Dictum Mi Limited| 17206.62483046548|\n",
       "|Donec Luctus Indu...| 4460.976807898458|\n",
       "|Elit Sed Consequa...|14657.495225952076|\n",
       "|Hendrerit Consect...|  2867.77080507758|\n",
       "+--------------------+------------------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model.transform(outdata1)\n",
    "\n",
    "predictions_test.createOrReplaceTempView(\"preds\")\n",
    "\n",
    "pred = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, SUM(prediction) AS total_revenue\n",
    "FROM preds\n",
    "GROUP BY merchant_name\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "pred.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred.toPandas()\n",
    "\n",
    "pred_df.to_csv(\"../data/curated/revenue.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "afcda84b7471a9b9fde108a34f159f021985318d3feb99cad4970c959fa9ac9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
