{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import year, month\n",
    "import pandas as pd\n",
    "import lbl2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 14:42:54 WARN Utils: Your hostname, MacBook-Air-3.local resolves to a loopback address: 127.0.0.1; using 192.168.0.66 instead (on interface en0)\n",
      "22/10/07 14:42:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 14:42:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/07 14:42:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kasturi/opt/anaconda3/lib/python3.9/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 14:42:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read in data from ETL.py file\n",
    "%run '../scripts/ETL.py' '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_merchants_sdf = spark.read.parquet(\"../data/curated/tagged_merchants.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_merchants_sdf = tagged_merchants_sdf.withColumnRenamed('merchant_abn',\n",
    "\n",
    "    'tagged_merchant_abn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 14:46:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 14:49:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%run '../scripts/outlier.py' '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal4.createOrReplaceTempView(\"join\")\n",
    "tagged_merchants_sdf.createOrReplaceTempView(\"tagged\")\n",
    "\n",
    "joint = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM join\n",
    "INNER JOIN tagged\n",
    "ON join.merchant_abn = tagged.tagged_merchant_abn\n",
    "\"\"\")\n",
    "\n",
    "joint = joint.drop('tagged_merchant_abn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint.createOrReplaceTempView(\"group\")\n",
    "\n",
    "main_data = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *, ((take_rate/100)*dollar_value) AS percent\n",
    "FROM group\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year, month, day from the timestamp\n",
    "\n",
    "\n",
    "main_data = main_data.withColumn('Year', year(main_data.order_datetime))\n",
    "main_data = main_data.withColumn('Month',month(main_data.order_datetime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = main_data.drop('merchant_abn', 'categories','name', 'address', 'trans_merchant_abn', 'order_id','order_datetime','user_id',\n",
    "'consumer_id','int_sa2','SA2_name','state_code','state_name','population_2020', 'population_2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_name</th><th>take_rate</th><th>revenue_levels</th><th>state</th><th>gender</th><th>dollar_value</th><th>suburb</th><th>postcodes</th><th>long</th><th>lat</th><th>SA2_code</th><th>income_2018-2019</th><th>total_males</th><th>total_females</th><th>total_persons</th><th>fraud_probability_consumer</th><th>fraud_probability_merchant</th><th>category</th><th>percent</th><th>Year</th><th>Month</th></tr>\n",
       "<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+---------+--------------+-----+------+------------+------+---------+----+---+--------+----------------+-----------+-------------+-------------+--------------------------+--------------------------+--------+-------+----+-----+\n",
       "|merchant_name|take_rate|revenue_levels|state|gender|dollar_value|suburb|postcodes|long|lat|SA2_code|income_2018-2019|total_males|total_females|total_persons|fraud_probability_consumer|fraud_probability_merchant|category|percent|Year|Month|\n",
       "+-------------+---------+--------------+-----+------+------------+------+---------+----+---+--------+----------------+-----------+-------------+-------------+--------------------------+--------------------------+--------+-------+----+-----+\n",
       "|            0|        0|             0|    0|     0|           0|     0|        0|   0|  0|       0|               0|          0|            0|            0|                         0|                         0|       0|      0|   0|    0|\n",
       "+-------------+---------+--------------+-----+------+------------+------+---------+----+---+--------+----------------+-----------+-------------+-------------+--------------------------+--------------------------+--------+-------+----+-----+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Find Count of Null, None, NaN of All DataFrame Columns\n",
    "main_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in main_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "male = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS m_name, COUNT(gender) as males\n",
    "FROM agg\n",
    "WHERE gender = 'Male'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "female = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS f_name, COUNT(gender) as females\n",
    "FROM agg\n",
    "WHERE gender = 'Female'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "main_agg_data = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, COUNT(merchant_name) AS no_of_transactions, SA2_code, Year, Month, SUM(dollar_value - percent) AS total_earnings,\n",
    "    CONCAT(merchant_name, SA2_code, Year, Month) AS join_col\n",
    "FROM agg\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_agg_data.createOrReplaceTempView(\"gender_join\")\n",
    "male.createOrReplaceTempView(\"male_agg\")\n",
    "female.createOrReplaceTempView(\"female_agg\")\n",
    "\n",
    "temp = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM gender_join\n",
    "INNER JOIN male_agg\n",
    "ON gender_join.join_col = male_agg.m_name\n",
    "\"\"\")\n",
    "\n",
    "temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "gender_agg = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM temp\n",
    "INNER JOIN female_agg\n",
    "ON temp.join_col = female_agg.f_name\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = main_data.withColumnRenamed('income_2018-2019',\n",
    "\n",
    "    'income_2018_2019'    \n",
    ")\n",
    "\n",
    "main_data = main_data.withColumn('income_per_persons',\n",
    "    (F.col('income_2018_2019')/F.col('total_persons'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.createOrReplaceTempView(\"features\")\n",
    "\n",
    "other_agg = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name AS drop_name, FIRST(take_rate) AS take_rate, FIRST(revenue_levels) AS revenue_levels, FIRST(category) AS category,\n",
    "    FIRST(total_males) AS males_in_SA2, FIRST(total_females) AS females_in_SA2, FIRST(income_per_persons) AS income_per_person\n",
    "FROM features\n",
    "GROUP BY merchant_name\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_name</th><th>no_of_transactions</th><th>SA2_code</th><th>Year</th><th>Month</th><th>total_earnings</th><th>males</th><th>females</th><th>take_rate</th><th>revenue_levels</th><th>category</th><th>males_in_SA2</th><th>females_in_SA2</th><th>income_per_person</th></tr>\n",
       "<tr><td>A Aliquet Ltd</td><td>2</td><td>312021351</td><td>2021</td><td>6</td><td>298.8537411028336</td><td>1</td><td>1</td><td>3.87</td><td>b</td><td>Furniture</td><td>3292</td><td>3206</td><td>28693.71558221812</td></tr>\n",
       "<tr><td>A Aliquet Ltd</td><td>2</td><td>401021010</td><td>2021</td><td>4</td><td>670.4857203238805</td><td>1</td><td>1</td><td>3.87</td><td>b</td><td>Furniture</td><td>3292</td><td>3206</td><td>28693.71558221812</td></tr>\n",
       "<tr><td>A Aliquet Ltd</td><td>2</td><td>603011065</td><td>2021</td><td>12</td><td>346.5799250465364</td><td>1</td><td>1</td><td>3.87</td><td>b</td><td>Furniture</td><td>3292</td><td>3206</td><td>28693.71558221812</td></tr>\n",
       "<tr><td>A Arcu Industries</td><td>2</td><td>124011453</td><td>2021</td><td>8</td><td>203.5414742977921</td><td>1</td><td>1</td><td>3.0</td><td>c</td><td>Furniture</td><td>4821</td><td>4683</td><td>25816.03452631579</td></tr>\n",
       "<tr><td>A Arcu Industries</td><td>2</td><td>211051282</td><td>2022</td><td>3</td><td>655.1195924003883</td><td>1</td><td>1</td><td>3.0</td><td>c</td><td>Furniture</td><td>4821</td><td>4683</td><td>25816.03452631579</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+------------------+---------+----+-----+-----------------+-----+-------+---------+--------------+---------+------------+--------------+-----------------+\n",
       "|    merchant_name|no_of_transactions| SA2_code|Year|Month|   total_earnings|males|females|take_rate|revenue_levels| category|males_in_SA2|females_in_SA2|income_per_person|\n",
       "+-----------------+------------------+---------+----+-----+-----------------+-----+-------+---------+--------------+---------+------------+--------------+-----------------+\n",
       "|    A Aliquet Ltd|                 2|312021351|2021|    6|298.8537411028336|    1|      1|     3.87|             b|Furniture|        3292|          3206|28693.71558221812|\n",
       "|    A Aliquet Ltd|                 2|401021010|2021|    4|670.4857203238805|    1|      1|     3.87|             b|Furniture|        3292|          3206|28693.71558221812|\n",
       "|    A Aliquet Ltd|                 2|603011065|2021|   12|346.5799250465364|    1|      1|     3.87|             b|Furniture|        3292|          3206|28693.71558221812|\n",
       "|A Arcu Industries|                 2|124011453|2021|    8|203.5414742977921|    1|      1|      3.0|             c|Furniture|        4821|          4683|25816.03452631579|\n",
       "|A Arcu Industries|                 2|211051282|2022|    3|655.1195924003883|    1|      1|      3.0|             c|Furniture|        4821|          4683|25816.03452631579|\n",
       "+-----------------+------------------+---------+----+-----+-----------------+-----+-------+---------+--------------+---------+------------+--------------+-----------------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_agg.createOrReplaceTempView(\"edit\")\n",
    "other_agg.createOrReplaceTempView(\"rates\")\n",
    "\n",
    "other_cols = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM edit\n",
    "INNER JOIN rates\n",
    "ON edit.merchant_name = rates.drop_name\n",
    "\"\"\")\n",
    "\n",
    "train = other_cols.drop('m_name', 'f_name', 'drop_name','join_col')\n",
    "\n",
    "train.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_projection = train.select(\"merchant_name\", \"SA2_code\", \"Year\", \"Month\", 'total_earnings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_projection = train_projection.withColumn(\"prev_year\", \\\n",
    "              when(train_projection[\"Month\"] == 1, train_projection['Year'] - 1).otherwise(train_projection['Year']))\n",
    "train_projection = train_projection.withColumn(\"prev_month\", \\\n",
    "              when(train_projection[\"Month\"] == 1, 12).otherwise(train_projection['Month'] - 1))\n",
    "train_projection = train_projection.drop(\"Year\", \"Month\")\n",
    "train_projection = train_projection.withColumnRenamed(\"total_earnings\", \"future_earnings\") \\\n",
    "                            .withColumnRenamed(\"merchant_name\", \"p_merchant_name\") \\\n",
    "                            .withColumnRenamed(\"SA2_code\", \"p_SA2_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data= train.join(train_projection, (train.merchant_name == train_projection.p_merchant_name) & \n",
    "                           (train.SA2_code == train_projection.p_SA2_code) & \n",
    "                           (train.Year == train_projection.prev_year) & \n",
    "                           (train.Month == train_projection.prev_month), how = 'inner')\n",
    "\n",
    "final_data = final_data.drop(\"p_merchant_name\", \"p_SA2_code\",\"prev_year\", \"prev_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_str = ['Year', 'Month', 'SA2_code']\n",
    "\n",
    "for cols in field_str:\n",
    "    final_data = final_data.withColumn(cols,\n",
    "\n",
    "    F.col(cols).cast('STRING')\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "field_int = ['no_of_transactions', 'males', 'females', 'males_in_SA2', 'females_in_SA2']\n",
    "\n",
    "for col in field_int:\n",
    "    final_data = final_data.withColumn(col,\n",
    "\n",
    "    F.col(col).cast('INT')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# String indexing the categorical columns\n",
    "\n",
    "indexer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', 'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', 'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data = indexer.fit(final_data).transform(final_data)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', 'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1 = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', 'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', 'income_per_person', 'no_of_transactions','take_rate', 'total_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1 = assembler1.transform(onehotdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the target column as label\n",
    "\n",
    "outdata1 = outdata1.withColumnRenamed(\n",
    "    \"future_earnings\",\n",
    "    \"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assembling the features as a feature vector \n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1)\n",
    "\n",
    "outdata1 = featureIndexer.transform(outdata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "\n",
    "trainingData, testData = outdata1.randomSplit([0.7, 0.3], seed = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 658:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:10:54 WARN DAGScheduler: Broadcasting large task binary with size 1435.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:10:55 WARN DAGScheduler: Broadcasting large task binary with size 1435.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:10:57 WARN DAGScheduler: Broadcasting large task binary with size 1439.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:10:59 WARN DAGScheduler: Broadcasting large task binary with size 1552.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:11:06 WARN DAGScheduler: Broadcasting large task binary with size 1613.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:11:10 WARN DAGScheduler: Broadcasting large task binary with size 1734.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:11:13 WARN DAGScheduler: Broadcasting large task binary with size 1976.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:11:17 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "\n",
    "# Train model.  \n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_validation = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 919:=========================>                               (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:12:53 WARN DAGScheduler: Broadcasting large task binary with size 1450.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|        prediction|             label|            features|\n",
      "+------------------+------------------+--------------------+\n",
      "|313.19989310826276| 175.5058926208255|(2085,[209,1093,2...|\n",
      "|323.33586610805185| 184.5112457341748|(2085,[209,988,20...|\n",
      "|323.33586610805185| 273.6255392020826|(2085,[209,976,20...|\n",
      "| 337.6340307168625| 421.7745544721037|(2085,[209,989,20...|\n",
      "|323.33586610805185|152.81812360693857|(2085,[209,1039,2...|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1027:==================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:14:13 WARN DAGScheduler: Broadcasting large task binary with size 1440.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1048:>                                                       (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:14:15 WARN DAGScheduler: Broadcasting large task binary with size 1441.9 KiB\n",
      "Root Mean Squared Error (RMSE) on train data = 338.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1136:===============================>                        (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:15:26 WARN DAGScheduler: Broadcasting large task binary with size 1440.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1157:>                                                       (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 15:15:28 WARN DAGScheduler: Broadcasting large task binary with size 1441.8 KiB\n",
      "Mean Absolutee Error (MAE) on train data = 208.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the validation set \n",
    "\n",
    "predictions_validation.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "\n",
    "evaluator_train_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_train = evaluator_train_rmse.evaluate(predictions_validation)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data = %g\" % rmse_train)\n",
    "\n",
    "evaluator_train_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae_train = evaluator_train_mae.evaluate(predictions_validation)\n",
    "print(\"Mean Absolutee Error (MAE) on train data = %g\" % mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>idx</th><th>name</th><th>score</th></tr>\n",
       "<tr><td>2084</td><td>total_earnings</td><td>0.706494253861606</td></tr>\n",
       "<tr><td>2082</td><td>no_of_transactions</td><td>0.11835657363957608</td></tr>\n",
       "<tr><td>2074</td><td>category_vec_Book...</td><td>0.03577852283073161</td></tr>\n",
       "<tr><td>2078</td><td>category_vec_Beau...</td><td>0.03243474989330104</td></tr>\n",
       "<tr><td>2079</td><td>males_in_SA2</td><td>0.009616270196539918</td></tr>\n",
       "<tr><td>2076</td><td>category_vec_Furn...</td><td>0.009323740478788638</td></tr>\n",
       "<tr><td>2083</td><td>take_rate</td><td>0.008311122778563933</td></tr>\n",
       "<tr><td>88</td><td>merchant_name_vec...</td><td>0.008013061972634943</td></tr>\n",
       "<tr><td>975</td><td>SA2_code_vec_4060...</td><td>0.006946303591320976</td></tr>\n",
       "<tr><td>2080</td><td>females_in_SA2</td><td>0.00656990606509404</td></tr>\n",
       "<tr><td>110</td><td>merchant_name_vec...</td><td>0.006445796617212...</td></tr>\n",
       "<tr><td>20</td><td>merchant_name_vec...</td><td>0.005221478439642211</td></tr>\n",
       "<tr><td>2066</td><td>Month_vec_12</td><td>0.005178198287984697</td></tr>\n",
       "<tr><td>2081</td><td>income_per_person</td><td>0.004705653813325203</td></tr>\n",
       "<tr><td>226</td><td>merchant_name_vec...</td><td>0.004149564099183946</td></tr>\n",
       "<tr><td>69</td><td>merchant_name_vec...</td><td>0.003766867306613...</td></tr>\n",
       "<tr><td>588</td><td>merchant_name_vec...</td><td>0.002768311999448...</td></tr>\n",
       "<tr><td>62</td><td>merchant_name_vec...</td><td>0.002459909671021...</td></tr>\n",
       "<tr><td>976</td><td>SA2_code_vec_5090...</td><td>0.002343141760167585</td></tr>\n",
       "<tr><td>502</td><td>merchant_name_vec...</td><td>0.00208946373644362</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----+--------------------+--------------------+\n",
       "| idx|                name|               score|\n",
       "+----+--------------------+--------------------+\n",
       "|2084|      total_earnings|   0.706494253861606|\n",
       "|2082|  no_of_transactions| 0.11835657363957608|\n",
       "|2074|category_vec_Book...| 0.03577852283073161|\n",
       "|2078|category_vec_Beau...| 0.03243474989330104|\n",
       "|2079|        males_in_SA2|0.009616270196539918|\n",
       "|2076|category_vec_Furn...|0.009323740478788638|\n",
       "|2083|           take_rate|0.008311122778563933|\n",
       "|  88|merchant_name_vec...|0.008013061972634943|\n",
       "| 975|SA2_code_vec_4060...|0.006946303591320976|\n",
       "|2080|      females_in_SA2| 0.00656990606509404|\n",
       "| 110|merchant_name_vec...|0.006445796617212...|\n",
       "|  20|merchant_name_vec...|0.005221478439642211|\n",
       "|2066|        Month_vec_12|0.005178198287984697|\n",
       "|2081|   income_per_person|0.004705653813325203|\n",
       "| 226|merchant_name_vec...|0.004149564099183946|\n",
       "|  69|merchant_name_vec...|0.003766867306613...|\n",
       "| 588|merchant_name_vec...|0.002768311999448...|\n",
       "|  62|merchant_name_vec...|0.002459909671021...|\n",
       "| 976|SA2_code_vec_5090...|0.002343141760167585|\n",
       "| 502|merchant_name_vec...| 0.00208946373644362|\n",
       "+----+--------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ExtractFeatureImportance(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "  \n",
    "  \n",
    "#ExtractFeatureImportance(model.stages[-1].featureImportances, dataset, \"features\")\n",
    "dataset_fi = ExtractFeatureImportance(model.featureImportances, predictions_validation, \"features\")\n",
    "dataset_fi = spark.createDataFrame(dataset_fi)\n",
    "display(dataset_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "latest_year = train.select(max('Year')).collect()[0][0]\n",
    "agg_month_1 = train.filter(train.Year == latest_year)\n",
    "latest_month = agg_month_1.select(max('Month')).collect()[0][0]\n",
    "predicting_data = agg_month_1.filter(train.Month == latest_month)\n",
    "predicting_data = predicting_data.withColumn(\"future_earnings\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# String indexing the categorical columns\n",
    "\n",
    "indexer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', 'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', 'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data = indexer.fit(predicting_data).transform(predicting_data)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', 'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1 = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', 'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', 'income_per_person', 'no_of_transactions','take_rate', 'total_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1 = assembler1.transform(onehotdata)\n",
    "\n",
    "# Renaming the target column as label\n",
    "\n",
    "outdata1 = outdata1.withColumnRenamed(\n",
    "    \"future_earnings\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "\n",
    "# Assembling the features as a feature vector \n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1)\n",
    "\n",
    "outdata1 = featureIndexer.transform(outdata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_name</th><th>total_revenue</th></tr>\n",
       "<tr><td>Dictum Mi Incorpo...</td><td>651.5062366752677</td></tr>\n",
       "<tr><td>Dictum Mi Limited</td><td>17590.66839023223</td></tr>\n",
       "<tr><td>Donec Luctus Indu...</td><td>4560.543656726874</td></tr>\n",
       "<tr><td>Elit Sed Consequa...</td><td>14984.643443531158</td></tr>\n",
       "<tr><td>Hendrerit Consect...</td><td>2931.778065038705</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------------+\n",
       "|       merchant_name|     total_revenue|\n",
       "+--------------------+------------------+\n",
       "|Dictum Mi Incorpo...| 651.5062366752677|\n",
       "|   Dictum Mi Limited| 17590.66839023223|\n",
       "|Donec Luctus Indu...| 4560.543656726874|\n",
       "|Elit Sed Consequa...|14984.643443531158|\n",
       "|Hendrerit Consect...| 2931.778065038705|\n",
       "+--------------------+------------------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model.transform(outdata1)\n",
    "\n",
    "predictions_test.createOrReplaceTempView(\"preds\")\n",
    "\n",
    "pred = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, SUM(prediction) AS total_revenue\n",
    "FROM preds\n",
    "GROUP BY merchant_name\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "pred.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pred_df = pred.toPandas()\n",
    "\n",
    "pred_df.to_csv(\"../data/curated/revenue.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "afcda84b7471a9b9fde108a34f159f021985318d3feb99cad4970c959fa9ac9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
