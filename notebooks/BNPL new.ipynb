{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:41:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/08 15:41:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/08 15:41:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import lbl2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#==============================================================================\n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2 part 9\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kasturi/opt/anaconda3/lib/python3.9/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:41:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read in data from ETL.py file\n",
    "%run '../scripts/ETL.py' '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:43:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read in data from ETL.py file\n",
    "%run '../scripts/outlier.py' '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:44:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#==============================================================================\n",
    "# STEP 1: Prepare the main dataset\n",
    "#==============================================================================\n",
    "# Read the tagged model\n",
    "tagged_merchants_sdf = spark.read.parquet(\"../data/curated/tagged_merchants.parquet\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Rename the merchant column \n",
    "tagged_merchants_sdf = tagged_merchants_sdf.withColumnRenamed('merchant_abn',\n",
    "\n",
    "    'tagged_merchant_abn'\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the final dataset to the tagged model\n",
    "internal4.createOrReplaceTempView(\"join\")\n",
    "tagged_merchants_sdf.createOrReplaceTempView(\"tagged\")\n",
    "\n",
    "joint = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM join\n",
    "INNER JOIN tagged\n",
    "ON join.merchant_abn = tagged.tagged_merchant_abn\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculate the BNPL earnings \n",
    "joint = joint.drop('tagged_merchant_abn')\n",
    "joint.createOrReplaceTempView(\"group\")\n",
    "\n",
    "main_data = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *, ((take_rate/100)*dollar_value) AS BNPL_earnings\n",
    "FROM group\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Extracting the year, month, day from the timestamp\n",
    "main_data = main_data.withColumn('Year', year(main_data.order_datetime))\n",
    "main_data = main_data.withColumn('Month',month(main_data.order_datetime))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop the unwanted columns\n",
    "main_data = main_data.drop('merchant_abn', 'categories','name', 'address', \n",
    "'trans_merchant_abn', 'order_id','order_datetime','user_id','consumer_id',\n",
    "'int_sa2', 'SA2_name','state_code','state_name','population_2020', \n",
    "'population_2021')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Find Count of Null, None, NaN of All DataFrame Columns\n",
    "null_values = main_data.select([count(when(isnan(c) | col(c).isNull(), \n",
    "c)).alias(c) for c in main_data.columns])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculate the number of male and females customers for each merchant\n",
    "main_data.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "male = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS m_name, \n",
    "    COUNT(gender) as males\n",
    "FROM agg\n",
    "WHERE gender = 'Male'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "female = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS f_name, \n",
    "    COUNT(gender) as females\n",
    "FROM agg\n",
    "WHERE gender = 'Female'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Aggregate the main dataset\n",
    "main_data.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "main_agg = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, COUNT(merchant_name) AS no_of_transactions, SA2_code, \n",
    "Year, Month, SUM(BNPL_earnings) AS BNPL_earnings,\n",
    "    CONCAT(merchant_name, SA2_code, Year, Month) AS join_col\n",
    "FROM agg\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the male and female customer counts to the main dataset\n",
    "main_agg.createOrReplaceTempView(\"gender_join\")\n",
    "male.createOrReplaceTempView(\"m\")\n",
    "female.createOrReplaceTempView(\"f\")\n",
    "\n",
    "temp2 = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM gender_join\n",
    "INNER JOIN m\n",
    "ON gender_join.join_col = m.m_name\n",
    "\"\"\")\n",
    "\n",
    "temp2.createOrReplaceTempView(\"temp2\")\n",
    "\n",
    "temp3 = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM temp2\n",
    "INNER JOIN f\n",
    "ON temp2.join_col = f.f_name\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Rename the column for better readability \n",
    "main_data = main_data.withColumnRenamed('income_2018-2019',\n",
    "\n",
    "    'income_2018_2019'    \n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculate the income per person for each SA2 code\n",
    "main_data = main_data.withColumn('income_per_persons',\n",
    "    (F.col('income_2018_2019')/F.col('total_persons'))\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Extract the revenue level and category for each merchant and total females,\n",
    "# males and income per person for each SA2 code\n",
    "main_data.createOrReplaceTempView(\"features\")\n",
    "\n",
    "e = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name AS drop_name, FIRST(take_rate) AS take_rate, \n",
    "    FIRST(revenue_levels) AS revenue_levels, FIRST(category) AS category, \n",
    "    FIRST(total_males) AS males_in_SA2, FIRST(total_females) AS females_in_SA2,\n",
    "    FIRST(income_per_persons) AS income_per_person\n",
    "FROM features\n",
    "GROUP BY merchant_name\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the extracted values to the main dataset\n",
    "temp3.createOrReplaceTempView(\"edit\")\n",
    "e.createOrReplaceTempView(\"rates\")\n",
    "\n",
    "temp4 = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM edit\n",
    "INNER JOIN rates\n",
    "ON edit.merchant_name = rates.drop_name\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop the redundant columns\n",
    "train = temp4.drop('m_name', 'f_name', 'drop_name','join_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# STEP 2: Prepare a train and test dataset by offsetting the months by 1\n",
    "#==============================================================================\n",
    "# ----------------------------------------------------------------------------\n",
    "# Select the main columns for offsetting\n",
    "train_projection = train.select(\"merchant_name\", \"SA2_code\", \"Year\", \"Month\", \n",
    "'BNPL_earnings')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Offset the dataset by 1 month\n",
    "\n",
    "# Offset the year by 1 if the month if the first month\n",
    "train_projection = train_projection.withColumn(\"prev_year\", \\\n",
    "              when(train_projection[\"Month\"] == 1, \n",
    "              train_projection['Year'] - 1).otherwise(train_projection['Year']))\n",
    "\n",
    "train_projection = train_projection.withColumn(\"prev_month\", \\\n",
    "              when(train_projection[\"Month\"] == 1, \n",
    "              12).otherwise(train_projection['Month'] - 1))\n",
    "\n",
    "# Drop the redundant columns\n",
    "train_projection = train_projection.drop(\"Year\", \"Month\")\n",
    "\n",
    "# Renam the columns\n",
    "train_projection = train_projection.withColumnRenamed(\"BNPL_earnings\", \n",
    "                \"future_earnings\") \\\n",
    "                .withColumnRenamed(\"merchant_name\", \"p_merchant_name\") \\\n",
    "                .withColumnRenamed(\"SA2_code\", \"p_SA2_code\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the offsetted values to the rest of the SA2 and aggregated values\n",
    "final_data = train.join(train_projection, \n",
    "            (train.merchant_name == train_projection.p_merchant_name) & \n",
    "            (train.SA2_code == train_projection.p_SA2_code) & \n",
    "            (train.Year == train_projection.prev_year) & \n",
    "            (train.Month == train_projection.prev_month), how = 'inner')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop the redundant columns\n",
    "final_data = final_data.drop(\"p_merchant_name\", \"p_SA2_code\",\"prev_year\", \n",
    "\"prev_month\")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Change the variable types\n",
    "field_str = ['Year', 'Month', 'SA2_code']\n",
    "\n",
    "for cols in field_str:\n",
    "    final_data = final_data.withColumn(cols,F.col(cols).cast('STRING'))\n",
    "\n",
    "field_int = ['no_of_transactions', 'males', 'females', 'males_in_SA2', \n",
    "'females_in_SA2']\n",
    "\n",
    "for col in field_int:\n",
    "    final_data = final_data.withColumn(col, F.col(col).cast('INT'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- no_of_transactions: integer (nullable = false)\n",
      " |-- SA2_code: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- BNPL_earnings: double (nullable = true)\n",
      " |-- males: integer (nullable = false)\n",
      " |-- females: integer (nullable = false)\n",
      " |-- take_rate: double (nullable = true)\n",
      " |-- revenue_levels: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- males_in_SA2: integer (nullable = true)\n",
      " |-- females_in_SA2: integer (nullable = true)\n",
      " |-- income_per_person: double (nullable = true)\n",
      " |-- future_earnings: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#==============================================================================\n",
    "# STEP 3: Build and train the Random Forrest Model\n",
    "#==============================================================================\n",
    "# String indexing the categorical columns\n",
    "\n",
    "indexer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', \n",
    "'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', \n",
    "'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data = indexer.fit(final_data).transform(final_data)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', \n",
    "'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1 = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', \n",
    "'income_per_person', 'no_of_transactions','take_rate', 'BNPL_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1 = assembler1.transform(onehotdata)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Renaming the target column as label\n",
    "\n",
    "outdata1 = outdata1.withColumnRenamed(\n",
    "    \"future_earnings\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Assembling the features as a feature vector \n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1)\n",
    "\n",
    "outdata1 = featureIndexer.transform(outdata1)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "trainingData, testData = outdata1.randomSplit([0.7, 0.3], seed = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 461:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:47:35 WARN DAGScheduler: Broadcasting large task binary with size 1433.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:47:36 WARN DAGScheduler: Broadcasting large task binary with size 1433.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:47:38 WARN DAGScheduler: Broadcasting large task binary with size 1437.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:47:39 WARN DAGScheduler: Broadcasting large task binary with size 1550.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:47:44 WARN DAGScheduler: Broadcasting large task binary with size 1612.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:47:48 WARN DAGScheduler: Broadcasting large task binary with size 1732.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:47:52 WARN DAGScheduler: Broadcasting large task binary with size 1974.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:47:55 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 722:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:49:05 WARN DAGScheduler: Broadcasting large task binary with size 1439.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 743:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:49:08 WARN DAGScheduler: Broadcasting large task binary with size 1440.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 831:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:50:23 WARN DAGScheduler: Broadcasting large task binary with size 1439.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 852:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 15:50:25 WARN DAGScheduler: Broadcasting large task binary with size 1440.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Train model.  \n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_validation = model.transform(testData)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Evaluate the validation set \n",
    "predictions_validation.select(\"prediction\", \"label\", \"features\")\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator_train_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_train = evaluator_train_rmse.evaluate(predictions_validation)\n",
    "\n",
    "evaluator_train_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae_train = evaluator_train_mae.evaluate(predictions_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Define a function to extract important feature column names\n",
    "def ExtractFeatureImportance(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol\n",
    "                    ].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "  \n",
    "# ----------------------------------------------------------------------------- \n",
    "#ExtractFeatureImportance(model.stages[-1].featureImportances, dataset, \n",
    "# \"features\")\n",
    "dataset_fi = ExtractFeatureImportance(model.featureImportances, \n",
    "predictions_validation, \"features\")\n",
    "dataset_fi = spark.createDataFrame(dataset_fi)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Select the latest month from the latest year in the dataset which will be\n",
    "# used as a test set for future predictions due to the offsetting done \n",
    "# previously\n",
    "\n",
    "latest_year = train.select(max('Year')).collect()[0][0]\n",
    "agg_month_1 = train.filter(train.Year == latest_year)\n",
    "latest_month = agg_month_1.select(max('Month')).collect()[0][0]\n",
    "predicting_data = agg_month_1.filter(train.Month == latest_month)\n",
    "predicting_data = predicting_data.withColumn(\"future_earnings\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the variable types\n",
    "field_str = ['Year', 'Month', 'SA2_code']\n",
    "\n",
    "for cols in field_str:\n",
    "    predicting_data = predicting_data.withColumn(cols,F.col(cols).cast('STRING'))\n",
    "\n",
    "field_int = ['no_of_transactions', 'males', 'females', 'males_in_SA2', \n",
    "'females_in_SA2']\n",
    "\n",
    "for col in field_int:\n",
    "    predicting_data = predicting_data.withColumn(col, F.col(col).cast('INT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- no_of_transactions: integer (nullable = false)\n",
      " |-- SA2_code: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- BNPL_earnings: double (nullable = true)\n",
      " |-- males: integer (nullable = false)\n",
      " |-- females: integer (nullable = false)\n",
      " |-- take_rate: double (nullable = true)\n",
      " |-- revenue_levels: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- males_in_SA2: integer (nullable = true)\n",
      " |-- females_in_SA2: integer (nullable = true)\n",
      " |-- income_per_person: double (nullable = true)\n",
      " |-- future_earnings: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicting_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# STEP 4: Make future predictions\n",
    "#==============================================================================\n",
    "# Repeat the indexing and vector assembling steps again for the test data\n",
    "# String indexing the categorical columns\n",
    "\n",
    "indexer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', \n",
    "'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', \n",
    "'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data = indexer.fit(predicting_data).transform(predicting_data)\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', \n",
    "'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1 = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', \n",
    "'income_per_person', 'no_of_transactions','take_rate', 'BNPL_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1 = assembler1.transform(onehotdata)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Renaming the target column as label\n",
    "\n",
    "outdata1 = outdata1.withColumnRenamed(\n",
    "    \"future_earnings\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Assembling the features as a feature vector \n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------- \n",
    "# Transform the test data\n",
    "outdata1 = featureIndexer.transform(outdata1)\n",
    "\n",
    "predictions_test = model.transform(outdata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------- \n",
    "# Aggregate the predictions to merchant level to get the predicted BNPL \n",
    "# earnings from each merchant\n",
    "predictions_test.createOrReplaceTempView(\"preds\")\n",
    "\n",
    "pred = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, SUM(prediction) AS total_earnings_of_BNPL\n",
    "FROM preds\n",
    "GROUP BY merchant_name\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------  \n",
    "# Convert the predictions to a pandas dataframe and save as a csv\n",
    "pred_df = pred.toPandas()\n",
    "\n",
    "pred_df.to_csv(\"../data/curated/BNPL_earnings.csv\")\n",
    "# ----------------------------------------------------------------------------- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afcda84b7471a9b9fde108a34f159f021985318d3feb99cad4970c959fa9ac9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
