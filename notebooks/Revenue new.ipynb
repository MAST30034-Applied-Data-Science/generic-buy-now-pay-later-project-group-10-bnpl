{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:41:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import lbl2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#==============================================================================\n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2 part 7\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:41:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%run \"../scripts/ETL.py\" '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:42:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%run \"../scripts/outlier.py\" '../scripts/paths.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# STEP 1: Prepare the main dataset\n",
    "#==============================================================================\n",
    "# Read the tagged model\n",
    "tagged_merchants_sdf = spark.read.parquet(\"../data/curated/tagged_merchants.parquet\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Rename the merchant column \n",
    "tagged_merchants_sdf = tagged_merchants_sdf.withColumnRenamed('merchant_abn',\n",
    "\n",
    "    'tagged_merchant_abn'\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the final dataset to the tagged model\n",
    "internal4.createOrReplaceTempView(\"join\")\n",
    "tagged_merchants_sdf.createOrReplaceTempView(\"tagged\")\n",
    "\n",
    "joint = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM join\n",
    "INNER JOIN tagged\n",
    "ON join.merchant_abn = tagged.tagged_merchant_abn\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joint.createOrReplaceTempView(\"group\")\n",
    "\n",
    "main_data = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *, ((take_rate/100)*dollar_value) AS percent\n",
    "FROM group\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Extracting the year, month, day from the timestamp\n",
    "main_data = main_data.withColumn('Year', year(main_data.order_datetime))\n",
    "main_data = main_data.withColumn('Month',month(main_data.order_datetime))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "main_data = main_data.drop('merchant_abn', 'categories','name', 'address', \n",
    "'trans_merchant_abn', 'order_id','order_datetime','user_id',\n",
    "'consumer_id','int_sa2','SA2_name','state_code','state_name','population_2020', \n",
    "'population_2021')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    " # Find Count of Null, None, NaN of All DataFrame Columns\n",
    "null_values = main_data.select([count(when(isnan(c) | col(c).isNull(), \n",
    "c)).alias(c) for c in main_data.columns])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Extracting the number transactions according to males and females\n",
    "main_data.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "male = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS m_name, \n",
    "    COUNT(gender) as males\n",
    "FROM agg\n",
    "WHERE gender = 'Male'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "female = spark.sql(\"\"\" \n",
    "\n",
    "SELECT CONCAT(merchant_name, SA2_code, Year, Month) AS f_name, \n",
    "    COUNT(gender) as females\n",
    "FROM agg\n",
    "WHERE gender = 'Female'\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Aggregating the main dataset\n",
    "main_data.createOrReplaceTempView(\"agg\")\n",
    "\n",
    "main_agg_data = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, COUNT(merchant_name) AS no_of_transactions, SA2_code, \n",
    "    Year, Month, SUM(dollar_value - percent) AS total_earnings,\n",
    "    CONCAT(merchant_name, SA2_code, Year, Month) AS join_col\n",
    "FROM agg\n",
    "GROUP BY merchant_name, SA2_code, Year, Month\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the count of transactions per gender to the main aggregated dataset\n",
    "main_agg_data.createOrReplaceTempView(\"gender_join\")\n",
    "male.createOrReplaceTempView(\"male_agg\")\n",
    "female.createOrReplaceTempView(\"female_agg\")\n",
    "\n",
    "temp = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM gender_join\n",
    "INNER JOIN male_agg\n",
    "ON gender_join.join_col = male_agg.m_name\n",
    "\"\"\")\n",
    "\n",
    "temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "gender_agg = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM temp\n",
    "INNER JOIN female_agg\n",
    "ON temp.join_col = female_agg.f_name\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Renaming the column\n",
    "main_data = main_data.withColumnRenamed('income_2018-2019',\n",
    "\n",
    "    'income_2018_2019'    \n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculating the income per person\n",
    "main_data = main_data.withColumn('income_per_persons',\n",
    "    (F.col('income_2018_2019')/F.col('total_persons'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Extracting the take rate and revenue levels for every merchant and number\n",
    "# of males, females and income per peroson for each SA2 code\n",
    "main_data.createOrReplaceTempView(\"features\")\n",
    "\n",
    "other_agg = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name AS drop_name, FIRST(take_rate) AS take_rate, \n",
    "    FIRST(revenue_levels) AS revenue_levels, FIRST(category) AS category,\n",
    "    FIRST(total_males) AS males_in_SA2, FIRST(total_females) AS females_in_SA2, \n",
    "    FIRST(income_per_persons) AS income_per_person\n",
    "FROM features\n",
    "GROUP BY merchant_name\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Joining the above extracted values to the main aggregated dataset\n",
    "gender_agg.createOrReplaceTempView(\"edit\")\n",
    "other_agg.createOrReplaceTempView(\"rates\")\n",
    "\n",
    "other_cols = spark.sql(\"\"\" \n",
    "\n",
    "SELECT *\n",
    "FROM edit\n",
    "INNER JOIN rates\n",
    "ON edit.merchant_name = rates.drop_name\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dropping the redundant columns\n",
    "train = other_cols.drop('m_name', 'f_name', 'drop_name','join_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#==============================================================================\n",
    "# STEP 2: Prepare a train and test dataset by offsetting the months by 1\n",
    "#==============================================================================\n",
    "# Select the main columns for offsetting\n",
    "train_projection = train.select(\"merchant_name\", \"SA2_code\", \"Year\", \"Month\", \n",
    "'total_earnings')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Offset the dataset by 1 month\n",
    "\n",
    "# Offset the year by 1 if the month if the first month\n",
    "train_projection = train_projection.withColumn(\"prev_year\", \\\n",
    "              when(train_projection[\"Month\"] == 1, \n",
    "              train_projection['Year'] - 1).otherwise(train_projection['Year'\n",
    "              ]))\n",
    "train_projection = train_projection.withColumn(\"prev_month\", \\\n",
    "              when(train_projection[\"Month\"] == 1, 12\n",
    "              ).otherwise(train_projection['Month'] - 1))\n",
    "train_projection = train_projection.drop(\"Year\", \"Month\")\n",
    "train_projection = train_projection.withColumnRenamed(\"total_earnings\", \n",
    "              \"future_earnings\") \\\n",
    "                            .withColumnRenamed(\"merchant_name\", \n",
    "              \"p_merchant_name\") \\\n",
    "                            .withColumnRenamed(\"SA2_code\", \"p_SA2_code\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Join the offsetted values to the rest of the SA2 and aggregated values\n",
    "final_data= train.join(train_projection, \n",
    "                (train.merchant_name == train_projection.p_merchant_name) & \n",
    "                (train.SA2_code == train_projection.p_SA2_code) & \n",
    "                (train.Year == train_projection.prev_year) & \n",
    "                (train.Month == train_projection.prev_month), how = 'inner')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop the redundant columns\n",
    "final_data = final_data.drop(\"p_merchant_name\", \"p_SA2_code\",\"prev_year\", \n",
    "\"prev_month\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Change the column types\n",
    "field_str = ['Year', 'Month', 'SA2_code']\n",
    "\n",
    "for cols in field_str:\n",
    "    final_data = final_data.withColumn(cols,\n",
    "\n",
    "    F.col(cols).cast('STRING')\n",
    "\n",
    ")\n",
    "\n",
    "field_int = ['no_of_transactions', 'males', 'females', 'males_in_SA2', \n",
    "'females_in_SA2']\n",
    "\n",
    "for col in field_int:\n",
    "    final_data = final_data.withColumn(col,\n",
    "\n",
    "    F.col(col).cast('INT')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# STEP 3: Build and train the Random Forrest Model\n",
    "#==============================================================================\n",
    "# String indexing the categorical columns\n",
    "\n",
    "indexer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', \n",
    "'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', \n",
    "'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data = indexer.fit(final_data).transform(final_data)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', \n",
    "'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1 = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', \n",
    "'income_per_person', 'no_of_transactions','take_rate', 'total_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1 = assembler1.transform(onehotdata)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Renaming the target column as label\n",
    "\n",
    "outdata1 = outdata1.withColumnRenamed(\n",
    "    \"future_earnings\",\n",
    "    \"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Assembling the features as a feature vector \n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1)\n",
    "\n",
    "outdata1 = featureIndexer.transform(outdata1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "\n",
    "trainingData, testData = outdata1.randomSplit([0.7, 0.3], seed = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 701:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:46:44 WARN DAGScheduler: Broadcasting large task binary with size 1435.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:46:45 WARN DAGScheduler: Broadcasting large task binary with size 1435.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:46:47 WARN DAGScheduler: Broadcasting large task binary with size 1439.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:46:48 WARN DAGScheduler: Broadcasting large task binary with size 1552.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:46:55 WARN DAGScheduler: Broadcasting large task binary with size 1613.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:46:59 WARN DAGScheduler: Broadcasting large task binary with size 1734.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:47:03 WARN DAGScheduler: Broadcasting large task binary with size 1973.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:47:07 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 962:===============================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:48:29 WARN DAGScheduler: Broadcasting large task binary with size 1440.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 983:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:48:32 WARN DAGScheduler: Broadcasting large task binary with size 1441.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1071:=================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:49:52 WARN DAGScheduler: Broadcasting large task binary with size 1440.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1092:>                                                       (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:49:54 WARN DAGScheduler: Broadcasting large task binary with size 1441.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Train model.  \n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Make predictions.\n",
    "predictions_validation = model.transform(testData)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Evaluate the validation set \n",
    "\n",
    "predictions_validation.select(\"prediction\", \"label\", \"features\")\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "\n",
    "evaluator_train_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_train_revenue = evaluator_train_rmse.evaluate(predictions_validation)\n",
    "\n",
    "\n",
    "evaluator_train_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae_train_revenue = evaluator_train_mae.evaluate(predictions_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Define a function to extract important feature column names\n",
    "def ExtractFeatureImportance(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol\n",
    "                        ].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "  \n",
    " # ---------------------------------------------------------------------------- \n",
    "#ExtractFeatureImportance(model.stages[-1].featureImportances, dataset, \n",
    "# \"features\")\n",
    "dataset_fi = ExtractFeatureImportance(model.featureImportances, \n",
    "predictions_validation, \"features\")\n",
    "dataset_fi_revenue = spark.createDataFrame(dataset_fi)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Select the latest month from the latest year in the dataset which will be\n",
    "# used as a test set for future predictions due to the offsetting done \n",
    "# previously\n",
    "latest_year = train.select(max('Year')).collect()[0][0]\n",
    "agg_month_1 = train.filter(train.Year == latest_year)\n",
    "latest_month = agg_month_1.select(max('Month')).collect()[0][0]\n",
    "predicting_data = agg_month_1.filter(train.Month == latest_month)\n",
    "predicting_data = predicting_data.withColumn(\"future_earnings\", lit(0))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Change the column types\n",
    "field_str = ['Year', 'Month', 'SA2_code']\n",
    "\n",
    "for cols in field_str:\n",
    "    predicting_data = predicting_data.withColumn(cols,\n",
    "\n",
    "    F.col(cols).cast('STRING')\n",
    "\n",
    ")\n",
    "\n",
    "field_int = ['no_of_transactions', 'males', 'females', 'males_in_SA2', \n",
    "'females_in_SA2']\n",
    "\n",
    "for col in field_int:\n",
    "    predicting_data = predicting_data.withColumn(col,\n",
    "\n",
    "    F.col(col).cast('INT')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- no_of_transactions: integer (nullable = false)\n",
      " |-- SA2_code: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- total_earnings: double (nullable = true)\n",
      " |-- males: integer (nullable = false)\n",
      " |-- females: integer (nullable = false)\n",
      " |-- take_rate: double (nullable = true)\n",
      " |-- revenue_levels: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- males_in_SA2: integer (nullable = true)\n",
      " |-- females_in_SA2: integer (nullable = true)\n",
      " |-- income_per_person: double (nullable = true)\n",
      " |-- future_earnings: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicting_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#==============================================================================\n",
    "# STEP 4: Make future predictions\n",
    "#==============================================================================\n",
    "# Repeat the indexing and vector assembling steps again for the test data\n",
    "# String indexing the categorical columns\n",
    "\n",
    "indexer = StringIndexer(inputCols = ['merchant_name', 'SA2_code', 'Year', \n",
    "'Month', 'revenue_levels','category'],\n",
    "outputCols = ['merchant_name_num', 'SA2_code_num', 'Year_num', 'Month_num', \n",
    "'revenue_levels_num','category_num'], handleInvalid=\"keep\")\n",
    "\n",
    "indexd_data = indexer.fit(predicting_data).transform(predicting_data)\n",
    "\n",
    "\n",
    "# Applying onehot encoding to the categorical data that is string indexed above\n",
    "encoder = OneHotEncoder(inputCols = ['merchant_name_num', 'SA2_code_num', \n",
    "'Year_num', 'Month_num', 'revenue_levels_num','category_num'],\n",
    "outputCols = ['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec'])\n",
    "\n",
    "onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "\n",
    "\n",
    "# Assembling the training data as a vector of features \n",
    "assembler1 = VectorAssembler(\n",
    "inputCols=['merchant_name_vec', 'SA2_code_vec', 'Year_vec', 'Month_vec', \n",
    "'revenue_levels_vec','category_vec','males_in_SA2','females_in_SA2', \n",
    "'income_per_person', 'no_of_transactions','take_rate', 'total_earnings'],\n",
    "outputCol= \"features\" )\n",
    "\n",
    "outdata1 = assembler1.transform(onehotdata)\n",
    "\n",
    "# Renaming the target column as label\n",
    "\n",
    "outdata1 = outdata1.withColumnRenamed(\n",
    "    \"future_earnings\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Assembling the features as a feature vector \n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", \n",
    "    outputCol=\"indexedFeatures\").fit(outdata1)\n",
    "\n",
    "outdata1 = featureIndexer.transform(outdata1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Predict the future values\n",
    "predictions_test = model.transform(outdata1)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Aggregate the predictions to merchant level to get the predicted total \n",
    "# earnings from each merchant\n",
    "predictions_test.createOrReplaceTempView(\"preds\")\n",
    "\n",
    "pred = spark.sql(\"\"\" \n",
    "\n",
    "SELECT merchant_name, SUM(prediction) AS total_revenue\n",
    "FROM preds\n",
    "GROUP BY merchant_name\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Convert to a pandas dataframe and save as a csv\n",
    "pred_df = pred.toPandas()\n",
    "pred_df.to_csv(\"../data/curated/revenue.csv\")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (267491980.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [31]\u001b[0;36m\u001b[0m\n\u001b[0;31m    override def close(): Unit = stop()\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def stop(): Unit = {\n",
    "  sparkContext.stop()\n",
    "}\n",
    "\n",
    "override \n",
    "def close(): Unit = stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afcda84b7471a9b9fde108a34f159f021985318d3feb99cad4970c959fa9ac9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
