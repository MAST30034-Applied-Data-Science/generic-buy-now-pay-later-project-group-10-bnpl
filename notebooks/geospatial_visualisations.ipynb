{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:04:03 WARN Utils: Your hostname, mast30034 resolves to a loopback address: 127.0.1.1; using 45.113.234.45 instead (on interface eth0)\n",
      "22/09/20 13:04:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:04:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/20 13:04:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/09/20 13:04:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/09/20 13:04:05 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:05:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:(114 + 64) / 184][Stage 43:>   (0 + 0) / 2][Stage 44:>  (0 + 0) / 64] \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:05:25 WARN TaskSetManager: Stage 44 contains a task of very large size (5555 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:==>                                                     (4 + 64) / 78]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:06:39 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:79)\n",
      "\tat org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:257)\n",
      "\tat org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:255)\n",
      "\tat org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:225)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:185)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2602/0x00000008410ed440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "22/09/20 13:06:39 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [SA2_code] optional int64 SA2_code {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:16\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,616\n",
      " }\n",
      " [SA2_name] optional binary SA2_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:68\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 307,960/33,668\n",
      " }\n",
      " [address] optional binary address (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:375\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 211,981/33,975\n",
      " }\n",
      " [categories] optional binary categories (STRING) {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:22367\n",
      "   data: initial: values:32204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 362,640/55,595\n",
      " }\n",
      " [consumer_id] optional int64 consumer_id {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:120\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,720\n",
      " }\n",
      " [dollar_value] optional double dollar_value {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:67080\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/100,680\n",
      " }\n",
      " [gender] optional binary gender (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:33\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 80,566/33,633\n",
      " }\n",
      " [geometry] optional binary geometry (STRING) {\n",
      "  r:0 bytes\n",
      "  d:64 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1249460\n",
      "   data: initial: values:384\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 172 slabs, 117,666,400 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 83 slabs, 934,486,505 bytes\n",
      "   total: 1,052,152,908/1,052,152,969\n",
      " }\n",
      " [income_2018-2019] optional int64 income_2018-2019 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:16\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,616\n",
      " }\n",
      " [int_sa2] optional int32 int_sa2 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 33,600/33,608\n",
      " }\n",
      " [merchant_abn] optional int64 merchant_abn {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:11744\n",
      "   data: initial: values:32204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 65,286/44,972\n",
      " }\n",
      " [merchant_name] optional binary merchant_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:35656\n",
      "   data: initial: values:32204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 201,641/68,884\n",
      " }\n",
      " [name] optional binary name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:254\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 141,184/33,854\n",
      " }\n",
      " [order_datetime] optional int32 order_datetime (DATE) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2424\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 33,600/36,024\n",
      " }\n",
      " [order_id] optional binary order_id (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:336000\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 336,000/369,600\n",
      " }\n",
      " [population_2020] optional int64 population_2020 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:16\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,616\n",
      " }\n",
      " [population_2021] optional int64 population_2021 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:16\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,616\n",
      " }\n",
      " [postcodes] optional binary postcodes (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:32\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,632\n",
      " }\n",
      " [revenue_levels] optional binary revenue_levels (STRING) {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:25\n",
      "   data: initial: values:32204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 41,133/33,253\n",
      " }\n",
      " [state] optional binary state (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:7\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 58,800/33,607\n",
      " }\n",
      " [state_code] optional int64 state_code {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,608\n",
      " }\n",
      " [state_name] optional binary state_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:19\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 159,600/33,619\n",
      " }\n",
      " [take_rate] optional double take_rate {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:4128\n",
      "   data: initial: values:32204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 65,286/37,356\n",
      " }\n",
      " [total_females] optional int64 total_females {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:16\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,616\n",
      " }\n",
      " [total_males] optional int64 total_males {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:16\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,616\n",
      " }\n",
      " [total_persons] optional int64 total_persons {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:16\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,616\n",
      " }\n",
      " [trans_merchant_abn] optional int64 trans_merchant_abn {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:12328\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/45,928\n",
      " }\n",
      " [user_id] optional int64 user_id {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:120\n",
      "   data: initial: values:33600\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 67,200/33,720\n",
      " }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:06:41 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "22/09/20 13:06:41 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/20 13:06:41 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [SA2_code] optional int64 SA2_code {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:112\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 96,949/48,661\n",
      " }\n",
      " [SA2_name] optional binary SA2_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:284\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 305,608/48,833\n",
      " }\n",
      " [address] optional binary address (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:3224\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 428 bytes\n",
      "   total: 323,577/52,052\n",
      " }\n",
      " [categories] optional binary categories (STRING) {\n",
      "  r:0 bytes\n",
      "  d:2488 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:39796\n",
      "   data: initial: values:46420\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 77,010 bytes\n",
      "   total: 602,606/165,714\n",
      " }\n",
      " [consumer_id] optional int64 consumer_id {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:984\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 428 bytes\n",
      "   total: 97,228/49,812\n",
      " }\n",
      " [dollar_value] optional double dollar_value {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:159704\n",
      "   data: initial: values:80000\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 4 slabs, 182,856 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 480,147 bytes\n",
      "   total: 576,947/663,003\n",
      " }\n",
      " [gender] optional binary gender (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:33\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 292 bytes\n",
      "   total: 113,501/48,725\n",
      " }\n",
      " [geometry] optional binary geometry (STRING) {\n",
      "  r:0 bytes\n",
      "  d:64 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1111993\n",
      "   data: initial: values:368\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 172 slabs, 70,569,800 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 658 slabs, 507,316,368 bytes\n",
      "   total: 577,886,171/577,886,232\n",
      " }\n",
      " [income_2018-2019] optional int64 income_2018-2019 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:112\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 96,949/48,661\n",
      " }\n",
      " [int_sa2] optional int32 int_sa2 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:56\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 48,549/48,605\n",
      " }\n",
      " [merchant_abn] optional int64 merchant_abn {\n",
      "  r:0 bytes\n",
      "  d:2488 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:22464\n",
      "   data: initial: values:46420\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 88,993 bytes\n",
      "   total: 183,107/160,365\n",
      " }\n",
      " [merchant_name] optional binary merchant_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:2488 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:68034\n",
      "   data: initial: values:46420\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 88,993 bytes\n",
      "   total: 380,323/205,935\n",
      " }\n",
      " [name] optional binary name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2087\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 428 bytes\n",
      "   total: 206,826/50,915\n",
      " }\n",
      " [order_datetime] optional int32 order_datetime (DATE) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2424\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 75,252 bytes\n",
      "   total: 123,652/126,076\n",
      " }\n",
      " [order_id] optional binary order_id (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:800000\n",
      "   data: initial: values:80000\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 4 slabs, 648,000 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 2,167,363 bytes\n",
      "   total: 2,651,363/2,815,363\n",
      " }\n",
      " [population_2020] optional int64 population_2020 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:112\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 96,949/48,661\n",
      " }\n",
      " [population_2021] optional int64 population_2021 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:112\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 96,949/48,661\n",
      " }\n",
      " [postcodes] optional binary postcodes (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:160\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 218 bytes\n",
      "   total: 97,018/48,778\n",
      " }\n",
      " [revenue_levels] optional binary revenue_levels (STRING) {\n",
      "  r:0 bytes\n",
      "  d:2488 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:25\n",
      "   data: initial: values:46420\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 26,672 bytes\n",
      "   total: 85,971/75,605\n",
      " }\n",
      " [state] optional binary state (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:27\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 126 bytes\n",
      "   total: 83,609/48,553\n",
      " }\n",
      " [state_code] optional int64 state_code {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:32\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 126 bytes\n",
      "   total: 96,926/48,558\n",
      " }\n",
      " [state_name] optional binary state_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:64\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 126 bytes\n",
      "   total: 153,845/48,590\n",
      " }\n",
      " [take_rate] optional double take_rate {\n",
      "  r:0 bytes\n",
      "  d:2488 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:4664\n",
      "   data: initial: values:46420\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 77,010 bytes\n",
      "   total: 171,124/130,582\n",
      " }\n",
      " [total_females] optional int64 total_females {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:112\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 96,949/48,661\n",
      " }\n",
      " [total_males] optional int64 total_males {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:112\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 96,949/48,661\n",
      " }\n",
      " [total_persons] optional int64 total_persons {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:112\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 149 bytes\n",
      "   total: 96,949/48,661\n",
      " }\n",
      " [trans_merchant_abn] optional int64 trans_merchant_abn {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:23952\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 90,252 bytes\n",
      "   total: 187,052/162,604\n",
      " }\n",
      " [user_id] optional int64 user_id {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:984\n",
      "   data: initial: values:48400\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 3 slabs, 428 bytes\n",
      "   total: 97,228/49,812\n",
      " }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:06:41 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/20 13:06:41 ERROR FileFormatWriter: Job job_20220920130539789615956710719170_0058 aborted.\n",
      "22/09/20 13:06:41 ERROR Executor: Exception in task 14.0 in stage 58.0 (TID 1676)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "22/09/20 13:06:42 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 14.0 in stage 58.0 (TID 1676),5,main]\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "22/09/20 13:06:42 WARN TaskSetManager: Lost task 14.0 in stage 58.0 (TID 1676) (vm-45-113-234-45.rc.cloud.unimelb.edu.au executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\n",
      "22/09/20 13:06:42 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@62ddbe3a rejected from java.util.concurrent.ThreadPoolExecutor@1117c098[Shutting down, pool size = 64, active threads = 64, queued tasks = 0, completed tasks = 1666]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:305)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR TaskSetManager: Task 14 in stage 58.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [SA2_code] optional int64 SA2_code {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,864\n",
      " }\n",
      " [SA2_name] optional binary SA2_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:62\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 209,250/37,902\n",
      " }\n",
      " [address] optional binary address (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:468\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 262,759/38,308\n",
      " }\n",
      " [categories] optional binary categories (STRING) {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24407\n",
      "   data: initial: values:36332\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 412,252/61,763\n",
      " }\n",
      " [consumer_id] optional int64 consumer_id {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:136\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,976\n",
      " }\n",
      " [dollar_value] optional double dollar_value {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:75472\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/113,312\n",
      " }\n",
      " [gender] optional binary gender (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:33\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 98,075/37,873\n",
      " }\n",
      " [geometry] optional binary geometry (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1509614\n",
      "   data: initial: values:328\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 92 slabs, 31,741,080 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 94 slabs, 547,110,946 bytes\n",
      "   total: 578,852,026/578,852,026\n",
      " }\n",
      " [income_2018-2019] optional int64 income_2018-2019 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,864\n",
      " }\n",
      " [int_sa2] optional int32 int_sa2 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:12\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 37,840/37,852\n",
      " }\n",
      " [merchant_abn] optional int64 merchant_abn {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:12520\n",
      "   data: initial: values:36332\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 73,680/49,876\n",
      " }\n",
      " [merchant_name] optional binary merchant_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:38101\n",
      "   data: initial: values:36332\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 228,439/75,457\n",
      " }\n",
      " [name] optional binary name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:295\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 163,820/38,135\n",
      " }\n",
      " [order_datetime] optional int32 order_datetime (DATE) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2424\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 37,840/40,264\n",
      " }\n",
      " [order_id] optional binary order_id (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:378400\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 378,400/416,240\n",
      " }\n",
      " [population_2020] optional int64 population_2020 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,864\n",
      " }\n",
      " [population_2021] optional int64 population_2021 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,864\n",
      " }\n",
      " [postcodes] optional binary postcodes (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,864\n",
      " }\n",
      " [revenue_levels] optional binary revenue_levels (STRING) {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:25\n",
      "   data: initial: values:36332\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 46,431/37,381\n",
      " }\n",
      " [state] optional binary state (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:7\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 66,220/37,847\n",
      " }\n",
      " [state_code] optional int64 state_code {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,848\n",
      " }\n",
      " [state_name] optional binary state_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:19\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 179,740/37,859\n",
      " }\n",
      " [take_rate] optional double take_rate {\n",
      "  r:0 bytes\n",
      "  d:1024 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:4160\n",
      "   data: initial: values:36332\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 73,680/41,516\n",
      " }\n",
      " [total_females] optional int64 total_females {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,864\n",
      " }\n",
      " [total_males] optional int64 total_males {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,864\n",
      " }\n",
      " [total_persons] optional int64 total_persons {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,864\n",
      " }\n",
      " [trans_merchant_abn] optional int64 trans_merchant_abn {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:13192\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/51,032\n",
      " }\n",
      " [user_id] optional int64 user_id {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:136\n",
      "   data: initial: values:37840\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 75,680/37,976\n",
      " }\n",
      "}\n",
      "\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Aborting job 619dbbc0-9520-4fb3-aecd-4ca1a949bf16.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 58.0 failed 1 times, most recent failure: Lost task 14.0 in stage 58.0 (TID 1676) (vm-45-113-234-45.rc.cloud.unimelb.edu.au executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n",
      "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305393589443243085692922_0058 aborted.\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305395736748121907223469_0058_m_000058_1720\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305395736748121907223469_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398767973342689067477_0058_m_000007_1669\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398767973342689067477_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305397390694610607380945_0058_m_000033_1695\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305397390694610607380945_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398922099535964824611_0058_m_000004_1666\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398922099535964824611_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305394156528748374214710_0058_m_000046_1708\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305394156528748374214710_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398686407307206740501_0058_m_000041_1703\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398686407307206740501_0058 aborted.\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305394714063764403829383_0058_m_000019_1681\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305394714063764403829383_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305392438083104742667478_0058_m_000043_1705\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305392438083104742667478_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305397283964457627614214_0058_m_000048_1710\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305397283964457627614214_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305394177470133649286191_0058_m_000049_1711\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305394177470133649286191_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305394746298912030023811_0058_m_000030_1692\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305394746298912030023811_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305392708623486572302954_0058_m_000000_1662\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305392708623486572302954_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305391030702467395294490_0058_m_000001_1663\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305391030702467395294490_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398880450338203528247_0058_m_000065_1727\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398880450338203528247_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396882128969398975289_0058_m_000036_1698\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396882128969398975289_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305399086038168198323239_0058_m_000013_1675\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305399086038168198323239_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305395868853465790481895_0058_m_000015_1677\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305395868853465790481895_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396471150431052121606_0058_m_000018_1680\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396471150431052121606_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305392817167525616819292_0058_m_000044_1706\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305392817167525616819292_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_20220920130539830151555477857208_0058_m_000054_1716\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_20220920130539830151555477857208_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305393886606613195004178_0058_m_000053_1715\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305393886606613195004178_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305391483622403412078619_0058_m_000035_1697\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305391483622403412078619_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398932008506302126923_0058_m_000005_1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398932008506302126923_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398479954565406789220_0058_m_000061_1723\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398479954565406789220_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305399030199524199589406_0058_m_000022_1684\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305399030199524199589406_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396930345454061939903_0058_m_000066_1728\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396930345454061939903_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396607130687577500092_0058_m_000037_1699\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396607130687577500092_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396862072796025202965_0058_m_000055_1717\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396862072796025202965_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398098987185327912081_0058_m_000050_1712\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398098987185327912081_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305393503246144572872610_0058_m_000009_1671\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305393503246144572872610_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305395326375642754904527_0058_m_000028_1690\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305395326375642754904527_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305392460469354996824270_0058_m_000067_1729\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305392460469354996824270_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305392009995984439938763_0058_m_000045_1707\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305392009995984439938763_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398495070191159862439_0058_m_000062_1724\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398495070191159862439_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305395264606424733834332_0058_m_000002_1664\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305395264606424733834332_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305393775351931390751910_0058_m_000031_1693\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305393775351931390751910_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305391854059758093491899_0058_m_000020_1682\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305391854059758093491899_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396951402202953380225_0058_m_000063_1725\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396951402202953380225_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305391226331560790796756_0058_m_000011_1673\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305391226331560790796756_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305393013717386274625274_0058_m_000052_1714\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305393013717386274625274_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396440417155301940352_0058_m_000006_1668\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396440417155301940352_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_20220920130539921448378917138976_0058_m_000047_1709\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_20220920130539921448378917138976_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305393397683685637873747_0058_m_000040_1702\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305393397683685637873747_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305394016076548607648456_0058_m_000003_1665\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305394016076548607648456_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396624726131582852388_0058_m_000038_1700\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305393680949879995618154_0058_m_000024_1686\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396624726131582852388_0058 aborted.\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305393680949879995618154_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305398188931278801006347_0058_m_000023_1685\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_20220920130539122681065009588807_0058_m_000039_1701\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_20220920130539122681065009588807_0058 aborted.\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305398188931278801006347_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305392025604712329988117_0058_m_000056_1718\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396870402709746503798_0058_m_000032_1694\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305392025604712329988117_0058 aborted.\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396870402709746503798_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305399073254970942051668_0058_m_000029_1691\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305399073254970942051668_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_20220920130539304400856604332905_0058_m_000026_1688\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_20220920130539304400856604332905_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305396545258983814913955_0058_m_000059_1721\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305396545258983814913955_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305395825848920139070548_0058_m_000051_1713\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305395825848920139070548_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305393076723769920396210_0058_m_000027_1689\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305393076723769920396210_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305394078212104375903906_0058_m_000025_1687\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305394078212104375903906_0058 aborted.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o226.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 58.0 failed 1 times, most recent failure: Lost task 14.0 in stage 58.0 (TID 1676) (vm-45-113-234-45.rc.cloud.unimelb.edu.au executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/scripts/ETL.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"postcode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m \u001b[0mfinal_join3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/tables/full_join.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     def text(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o226.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 58.0 failed 1 times, most recent failure: Lost task 14.0 in stage 58.0 (TID 1676) (vm-45-113-234-45.rc.cloud.unimelb.edu.au executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:167)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:168)\n\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:59)\n\tat org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:475)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4925/0x00000008415c1c40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4905/0x000000084159e040.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305397070092236383127960_0058_m_000057_1719\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305397070092236383127960_0058 aborted.\n",
      "22/09/20 13:06:42 WARN FileOutputCommitter: Could not delete file:/mnt/student.unimelb.edu.au/arshnoork/generic-buy-now-pay-later-project-group-10-bnpl/data/tables/full_join.parquet/_temporary/0/_temporary/attempt_202209201305395721491200147070912_0058_m_000021_1683\n",
      "22/09/20 13:06:42 ERROR FileFormatWriter: Job job_202209201305395721491200147070912_0058 aborted.\n",
      "22/09/20 13:06:42 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:706)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.smj_findNextJoinRows_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/20 13:06:42 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [SA2_code] optional int64 SA2_code {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:64\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 142,448/71,312\n",
      " }\n",
      " [SA2_name] optional binary SA2_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:149\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 254,197/71,397\n",
      " }\n",
      " [address] optional binary address (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1770\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 141 bytes\n",
      "   total: 480,078/73,111\n",
      " }\n",
      " [categories] optional binary categories (STRING) {\n",
      "  r:0 bytes\n",
      "  d:2392 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:34461\n",
      "   data: initial: values:68204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 25,664 bytes\n",
      "   total: 800,466/130,721\n",
      " }\n",
      " [consumer_id] optional int64 consumer_id {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:520\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 141 bytes\n",
      "   total: 142,541/71,861\n",
      " }\n",
      " [dollar_value] optional double dollar_value {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:159568\n",
      "   data: initial: values:80000\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 4 slabs, 182,856 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 160,049 bytes\n",
      "   total: 302,449/342,905\n",
      " }\n",
      " [gender] optional binary gender (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:33\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 105 bytes\n",
      "   total: 175,986/71,338\n",
      " }\n",
      " [geometry] optional binary geometry (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1140049\n",
      "   data: initial: values:292\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 348 slabs, 672,459,802 bytes\n",
      "   total: 672,459,802/672,459,802\n",
      " }\n",
      " [income_2018-2019] optional int64 income_2018-2019 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:64\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 142,448/71,312\n",
      " }\n",
      " [int_sa2] optional int32 int_sa2 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:32\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 71,248/71,280\n",
      " }\n",
      " [merchant_abn] optional int64 merchant_abn {\n",
      "  r:0 bytes\n",
      "  d:2392 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:19304\n",
      "   data: initial: values:68204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 28,061 bytes\n",
      "   total: 166,407/117,961\n",
      " }\n",
      " [merchant_name] optional binary merchant_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:2392 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:58295\n",
      "   data: initial: values:68204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 28,061 bytes\n",
      "   total: 455,023/156,952\n",
      " }\n",
      " [name] optional binary name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1132\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 141 bytes\n",
      "   total: 318,377/72,473\n",
      " }\n",
      " [order_datetime] optional int32 order_datetime (DATE) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2424\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 25,084 bytes\n",
      "   total: 96,284/98,708\n",
      " }\n",
      " [order_id] optional binary order_id (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:800000\n",
      "   data: initial: values:80000\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 5 slabs, 857,715 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 721,925 bytes\n",
      "   total: 1,433,925/1,579,640\n",
      " }\n",
      " [population_2020] optional int64 population_2020 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:64\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 142,448/71,312\n",
      " }\n",
      " [population_2021] optional int64 population_2021 {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:64\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 142,448/71,312\n",
      " }\n",
      " [postcodes] optional binary postcodes (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:80\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 60 bytes\n",
      "   total: 142,460/71,340\n",
      " }\n",
      " [revenue_levels] optional binary revenue_levels (STRING) {\n",
      "  r:0 bytes\n",
      "  d:2392 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:25\n",
      "   data: initial: values:68204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 8,878 bytes\n",
      "   total: 96,071/79,499\n",
      " }\n",
      " [state] optional binary state (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:14\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 39 bytes\n",
      "   total: 124,639/71,253\n",
      " }\n",
      " [state_code] optional int64 state_code {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:16\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 39 bytes\n",
      "   total: 142,439/71,255\n",
      " }\n",
      " [state_name] optional binary state_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:31\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 39 bytes\n",
      "   total: 218,245/71,270\n",
      " }\n",
      " [take_rate] optional double take_rate {\n",
      "  r:0 bytes\n",
      "  d:2392 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:4600\n",
      "   data: initial: values:68204\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 25,664 bytes\n",
      "   total: 164,010/100,860\n",
      " }\n",
      " [total_females] optional int64 total_females {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:64\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 142,448/71,312\n",
      " }\n",
      " [total_males] optional int64 total_males {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:64\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 142,448/71,312\n",
      " }\n",
      " [total_persons] optional int64 total_persons {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:64\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 48 bytes\n",
      "   total: 142,448/71,312\n",
      " }\n",
      " [trans_merchant_abn] optional int64 trans_merchant_abn {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:20472\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 30,083 bytes\n",
      "   total: 172,483/121,755\n",
      " }\n",
      " [user_id] optional int64 user_id {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:520\n",
      "   data: initial: values:71200\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 1 slabs, 141 bytes\n",
      "   total: 142,541/71,861\n",
      " }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 49536)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/miniconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/miniconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/miniconda3/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/student.unimelb.edu.au/arshnoork/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/student.unimelb.edu.au/arshnoork/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/student.unimelb.edu.au/arshnoork/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/student.unimelb.edu.au/arshnoork/.local/lib/python3.8/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%run \"../scripts/ETL.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(postcode='7252', count=2682)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_join2.head()\n",
    "num_transactions_by_postcode = final_join2.groupBy('postcode').count()\n",
    "num_transactions_by_postcode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3166"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_transactions_by_postcode.head()\n",
    "num_transactions_by_postcode.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_data1 = spark.read.option(\"header\", True).csv('../data/postcodes.csv')\n",
    "postcodes_data2 = postcodes_data1.withColumnRenamed('Postcode', 'postcode')\n",
    "postcodes_data = postcodes_data2.dropDuplicates(['postcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(postcode='2136', count=1026, Suburb='Burwood Heights', State='NSW', Lat='-33.890', Lon='151.100')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_location = num_transactions_by_postcode.join(postcodes_data,['postcode'], 'inner')\n",
    "transactions_location.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3136"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_location.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above - 30 postcodes in the transactions dataset were not found in the postcodes information dataset and hence their lattitude and longitude values were not retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2454"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundaries.head(5)\n",
    "len(boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The following was modified from MAST30034 Tutorial 2\n",
    "# read in shape file\n",
    "\n",
    "transactions_location_pdf = transactions_location.toPandas()\n",
    "\n",
    "# join transaction location df with shape file\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    pd.merge(transactions_location_pdf, boundaries, left_on='Suburb', right_on='SA2_NAME21')\n",
    ")\n",
    "\n",
    "len(gdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_coords = [-25.2744, 133.7751]\n",
    "m = folium.Map(aus_coords, tiles='OpenStreetMap', zoom_start=4.5)\n",
    "\n",
    "for index, row in transactions_location_pdf.iterrows():\n",
    "    if row['count'] >= 5000:\n",
    "        marker_color = 'darkred'\n",
    "        fill_color = 'darkred'\n",
    "    elif row['count'] >= 1000:\n",
    "        marker_color = 'red'\n",
    "        fill_color = 'red'\n",
    "    elif row['count'] >= 500:\n",
    "        marker_color = 'lightred'\n",
    "        fill_color = 'lightred'\n",
    "    elif row['count'] >= 100:\n",
    "        marker_color = 'orange'\n",
    "        fill_color = 'orange'\n",
    "    elif row['count'] <= 50 :\n",
    "        marker_color = 'yellow'\n",
    "        fill_color = 'yellow'\n",
    "    else:\n",
    "        marker_color='darkpurple'\n",
    "        fill_color = 'darkpurple'\n",
    "        \n",
    "    folium.Circle(\n",
    "          location=[row['Lat'], row['Lon']],\n",
    "          popup= 'Number of transactions: ' +str(row['count']),\n",
    "          tooltip=row['Suburb'],\n",
    "          radius=row['count'],\n",
    "          color=marker_color,\n",
    "          fill=True,\n",
    "          fill_color=fill_color,\n",
    "       ).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
